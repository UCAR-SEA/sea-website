<!DOCTYPE html><html lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="The public facing site for the UCAR SEA featuring news, event information, and best practices."><meta name=author content="SEA Steering Committee"><link href=https://UCAR-SEA.github.io/sea-website/iss/2025-program/ rel=canonical><link href=../2025/ rel=prev><link href=../2025-proceedings/ rel=next><link rel=icon href=../../assets/images/sea_icon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.21"><title>UCAR Software Engineering Assembly - Program</title><link rel=stylesheet href=../../assets/stylesheets/main.2a3383ac.min.css><link rel=stylesheet href=../../assets/stylesheets/palette.06af60db.min.css><link rel=stylesheet href=../../css/NCAR_primary.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../css/timeago.css><script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><link href=../../assets/stylesheets/glightbox.min.css rel=stylesheet><script src=../../assets/javascripts/glightbox.min.js></script><style id=glightbox-style>
            html.glightbox-open { overflow: initial; height: 100%; }
            .gslide-title { margin-top: 0px; user-select: text; }
            .gslide-desc { color: #666; user-select: text; }
            .gslide-image img { background: white; }
            .gscrollbar-fixer { padding-right: 15px; }
            .gdesc-inner { font-size: 0.75rem; }
            body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color); }
            body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color); }
            body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color); }
        </style></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#online-program class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <!--
  Copyright (c) 2016-2024 Martin Donath <martin.donath@squidfunk.com>

  Permission is hereby granted, free of charge, to any person obtaining a copy
  of this software and associated documentation files (the "Software"), to
  deal in the Software without restriction, including without limitation the
  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
  sell copies of the Software, and to permit persons to whom the Software is
  furnished to do so, subject to the following conditions:

  The above copyright notice and this permission notice shall be included in
  all copies or substantial portions of the Software.

  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE
  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
  IN THE SOFTWARE.
--> <!-- Determine classes --> <!-- Header --> <header class="md-header md-header--shadow md-header--lifted" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <!-- Link to home --> <a href=../.. title="UCAR Software Engineering Assembly" class="md-header__button md-logo" aria-label="UCAR Software Engineering Assembly" data-md-component=logo> <img src=../../assets/images/nsf-stackseal-logo-lockup-light.png alt=logo> </a> <!-- Button to open drawer --> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"></path></svg> </label> <!-- Header title --> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> </span> </div> </div> </div> <!-- Color palette toggle --> <!-- User preference: color palette --> <!-- Site language selector --> <!-- Button to open search modal --> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg> </label> <!-- Search interface --> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"></path></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <!-- Repository information --> <div class=md-header__source> <a href=https://github.com/UCAR-SEA/sea-website title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.0.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"></path></svg> </div> <div class=md-source__repository> UCAR-SEA/sea-website </div> </a> </div> </nav> <!-- Navigation tabs (sticky) --> <!--
  Copyright (c) 2016-2024 Martin Donath <martin.donath@squidfunk.com>

  Permission is hereby granted, free of charge, to any person obtaining a copy
  of this software and associated documentation files (the "Software"), to
  deal in the Software without restriction, including without limitation the
  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
  sell copies of the Software, and to permit persons to whom the Software is
  furnished to do so, subject to the following conditions:

  The above copyright notice and this permission notice shall be included in
  all copies or substantial portions of the Software.

  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE
  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
  IN THE SOFTWARE.
--> <!-- Navigation tabs --> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=site_title><p class=title_text>UCAR Software Engineering Assembly</p></li> <li class=md-tabs__item> <a href=../.. class=md-tabs__link> Home </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../ class=md-tabs__link> ISS Conference </a> </li> <li class=md-tabs__item> <a href=../../events/ class=md-tabs__link> Events </a> </li> <li class=md-tabs__item> <a href=../../community/ class=md-tabs__link> Community </a> </li> <li class=md-tabs__item> <a href=../../about/ class=md-tabs__link> About Us </a> </li> </ul> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../.. title="UCAR Software Engineering Assembly" class="md-nav__button md-logo" aria-label="UCAR Software Engineering Assembly" data-md-component=logo> <img src=../../assets/images/nsf-stackseal-logo-lockup-light.png alt=logo> </a> UCAR Software Engineering Assembly </label> <div class=md-nav__source> <a href=https://github.com/UCAR-SEA/sea-website title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.0.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"></path></svg> </div> <div class=md-source__repository> UCAR-SEA/sea-website </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_1> <div class="md-nav__link md-nav__container"> <a href=../.. class="md-nav__link "> <span class=md-ellipsis> Home </span> </a> <label class="md-nav__link " for=__nav_1 id=__nav_1_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_1_label aria-expanded=false> <label class=md-nav__title for=__nav_1> <span class="md-nav__icon md-icon"></span> Home </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_1_2> <label class=md-nav__link for=__nav_1_2 id=__nav_1_2_label tabindex=0> <span class=md-ellipsis> Archive </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_1_2_label aria-expanded=false> <label class=md-nav__title for=__nav_1_2> <span class="md-nav__icon md-icon"></span> Archive </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../archive/2025/ class=md-nav__link> <span class=md-ellipsis> 2025 </span> </a> </li> <li class=md-nav__item> <a href=../../archive/2024/ class=md-nav__link> <span class=md-ellipsis> 2024 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_1_3> <label class=md-nav__link for=__nav_1_3 id=__nav_1_3_label tabindex=0> <span class=md-ellipsis> Categories </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_1_3_label aria-expanded=false> <label class=md-nav__title for=__nav_1_3> <span class="md-nav__icon md-icon"></span> Categories </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../category/abstracts/ class=md-nav__link> <span class=md-ellipsis> Abstracts </span> </a> </li> <li class=md-nav__item> <a href=../../category/announcements/ class=md-nav__link> <span class=md-ellipsis> Announcements </span> </a> </li> <li class=md-nav__item> <a href=../../category/developer-exchange/ class=md-nav__link> <span class=md-ellipsis> Developer Exchange </span> </a> </li> <li class=md-nav__item> <a href=../../category/gpus/ class=md-nav__link> <span class=md-ellipsis> GPUs </span> </a> </li> <li class=md-nav__item> <a href=../../category/iss-conference/ class=md-nav__link> <span class=md-ellipsis> ISS Conference </span> </a> </li> <li class=md-nav__item> <a href=../../category/interoperability/ class=md-nav__link> <span class=md-ellipsis> Interoperability </span> </a> </li> <li class=md-nav__item> <a href=../../category/open-discussions/ class=md-nav__link> <span class=md-ellipsis> Open Discussions </span> </a> </li> <li class=md-nav__item> <a href=../../category/proceedings/ class=md-nav__link> <span class=md-ellipsis> Proceedings </span> </a> </li> <li class=md-nav__item> <a href=../../category/program/ class=md-nav__link> <span class=md-ellipsis> Program </span> </a> </li> <li class=md-nav__item> <a href=../../category/recordings/ class=md-nav__link> <span class=md-ellipsis> Recordings </span> </a> </li> <li class=md-nav__item> <a href=../../category/registration/ class=md-nav__link> <span class=md-ellipsis> Registration </span> </a> </li> <li class=md-nav__item> <a href=../../category/seminars/ class=md-nav__link> <span class=md-ellipsis> Seminars </span> </a> </li> <li class=md-nav__item> <a href=../../category/strategic-planning/ class=md-nav__link> <span class=md-ellipsis> Strategic Planning </span> </a> </li> <li class=md-nav__item> <a href=../../category/students/ class=md-nav__link> <span class=md-ellipsis> Students </span> </a> </li> <li class=md-nav__item> <a href=../../category/survey/ class=md-nav__link> <span class=md-ellipsis> Survey </span> </a> </li> <li class=md-nav__item> <a href=../../category/tools/ class=md-nav__link> <span class=md-ellipsis> Tools </span> </a> </li> <li class=md-nav__item> <a href=../../category/tutorials/ class=md-nav__link> <span class=md-ellipsis> Tutorials </span> </a> </li> <li class=md-nav__item> <a href=../../category/website/ class=md-nav__link> <span class=md-ellipsis> Website </span> </a> </li> <li class=md-nav__item> <a href=../../category/workshops/ class=md-nav__link> <span class=md-ellipsis> Workshops </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2 checked> <div class="md-nav__link md-nav__container"> <a href=../ class="md-nav__link "> <span class=md-ellipsis> ISS Conference </span> </a> <label class="md-nav__link " for=__nav_2 id=__nav_2_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=true> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> ISS Conference </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_2 checked> <label class=md-nav__link for=__nav_2_2 id=__nav_2_2_label tabindex> <span class=md-ellipsis> 2025 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_2_label aria-expanded=true> <label class=md-nav__title for=__nav_2_2> <span class="md-nav__icon md-icon"></span> 2025 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../2025/ class=md-nav__link> <span class=md-ellipsis> Conference </span> </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> Program </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> Program </span> </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#online-program class=md-nav__link> <span class=md-ellipsis> Online Program </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../2025-proceedings/ class=md-nav__link> <span class=md-ellipsis> Proceedings </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_2_3> <label class=md-nav__link for=__nav_2_3 id=__nav_2_3_label tabindex> <span class=md-ellipsis> 2024 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_3_label aria-expanded=false> <label class=md-nav__title for=__nav_2_3> <span class="md-nav__icon md-icon"></span> 2024 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../2024/ class=md-nav__link> <span class=md-ellipsis> Conference </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_3> <div class="md-nav__link md-nav__container"> <a href=../../events/ class="md-nav__link "> <span class=md-ellipsis> Events </span> </a> <label class="md-nav__link " for=__nav_3 id=__nav_3_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Events </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../events/discussions/ class=md-nav__link> <span class=md-ellipsis> Open Discussions </span> </a> </li> <li class=md-nav__item> <a href=../../events/seminars/ class=md-nav__link> <span class=md-ellipsis> Seminars </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_4> <div class="md-nav__link md-nav__container"> <a href=../../community/ class="md-nav__link "> <span class=md-ellipsis> Community </span> </a> <label class="md-nav__link " for=__nav_4 id=__nav_4_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> Community </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=https://ucarsea.slack.com class=md-nav__link> <span class=md-ellipsis> Slack </span> </a> </li> <li class=md-nav__item> <a href=../../community/listserv/ class=md-nav__link> <span class=md-ellipsis> Mailing List </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_5> <div class="md-nav__link md-nav__container"> <a href=../../about/ class="md-nav__link "> <span class=md-ellipsis> About Us </span> </a> <label class="md-nav__link " for=__nav_5 id=__nav_5_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_5_label aria-expanded=false> <label class=md-nav__title for=__nav_5> <span class="md-nav__icon md-icon"></span> About Us </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../archive/ class=md-nav__link> <span class=md-ellipsis> Legacy Site Archive </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <h1>Program</h1> <style>
.md-content .md-typeset h1 {
    display: none;
}
.md-typeset .admonition.cell {
    max-width: 500px;
    margin-inline: auto;
}
.md-typeset .admonition, .md-typeset details {
    border: 0;
    margin: 0 0 0 0;
}
.md-typeset .admonition > .admonition-title,
.md-typeset .cell > summary {
    background-color: rgba(255, 255, 255, 0.0);
    font-size: 15px;
}
.table {
    display: table;
    width: 100%;
}
.row {
    display: table-row;
    height: 50px;
}
.cell {
    display: table-cell;
    padding: 2px;
    padding-top: 15px;
    padding-right: 20px;
    vertical-align: middle;
    font-size: 15px;
}
.cell-content {
    display: table-cell;
    padding: 2px;
    width: 70%;
    vertical-align: middle;
}
</style> <figure> <img src=../../assets/iss2025.png alt=drawing width=80%> <figcaption><strong>From Legacy to Leading-Edge: Transforming Software Design in Science to Meet Tomorrow‚Äôs Challenges</strong></figcaption> </figure> <h2 id=online-program>Online Program<a class=headerlink href=#online-program title="Permanent link">üîóÔ∏é</a></h2> <div class="tabbed-set tabbed-alternate" data-tabs=1:4><input checked=checked id=__tabbed_1_1 name=__tabbed_1 type=radio><input id=__tabbed_1_2 name=__tabbed_1 type=radio><input id=__tabbed_1_3 name=__tabbed_1 type=radio><input id=__tabbed_1_4 name=__tabbed_1 type=radio><div class=tabbed-labels><label for=__tabbed_1_1>Monday, April 7</label><label for=__tabbed_1_2>Tuesday, April 8</label><label for=__tabbed_1_3>Wednesday, April 9</label><label for=__tabbed_1_4>Thursday, April 10</label></div> <div class=tabbed-content> <div class=tabbed-block> <p></p><div class=table> <div class=row> <div class=cell><b>8:30 AM</b></div> <div class=cell></div> <div class=cell-content><p></p> <details class="info cell"> <summary>Opening Remarks</summary> <p></p></details></div> </div> <div class=row> <div class=cell><b>8:40 AM</b></div> <div class=cell>Jacob Carley</div> <div class=cell-content><p></p> <details class="abstract cell"> <summary>Invited Keynote: Bringing Agility to the Operational Production Suite</summary> <p><strong>[<a href="https://drive.google.com/file/d/1gDAL4Sg_CMf3G9ado7g7Y8D1A7REz1tS/view?usp=drive_link">Slides</a>] - [<a href="https://www.youtube.com/watch?v=ErMtewdCY4s&t=760s">Recording</a>]</strong></p> <p><em>Jacob Carley earned his PhD in 2012 from Purdue University studying hybrid ensemble-3DVar methods for the assimilation of radar data under the support of an NSF Graduate Research Fellowship. Following his PhD he began as a Post Doc at the Environmental Modeling Center (EMC) working on data impact studies with the North American Mesoscale Forecast System (NAM). Later, as a contractor at EMC, he helped lead the last two operational implementations of the NAM. He also worked on the Real Time and UnRestricted Mesoscale Analysis systems and oversaw six operational implementations. He became a federal employee at EMC in 2018 in the Data Assimilation and Quality Control Group and led the development of NOAA‚Äôs next generation, convection-allowing ensemble prediction system, the Rapid Refresh Forecast System. In 2024 he became the Chief of the Engineering and Implementation Branch of EMC.</em></p> <p>The Environmental Modeling Center develops, maintains, and transitions analysis and prediction systems of the Earth system to operations. Over the past couple of decades the production suite has become increasingly complex with the advancement of science and technology. The development and implementation methods of the past are no longer suitable for the level of complexity of today‚Äôs systems. We now find ourselves with increasingly risky, high-stakes, big bang implementations that are susceptible to costly delays.</p> <p>An alternative approach is one that features more frequent, smaller upgrades underpinned by a robust foundation of proven agile methodologies, such as CI/CD and unit testing of the substantial repository of scientific software. This presentation will propose a restructuring of the entire development and implementation process in an effort to move toward a more sustainable and agile practice that meets the evolving needs of the National Weather Service‚Äôs mission. </p></details></div> </div> <div class=row> <div class=cell><b>9:50 AM</b></div> <div class=cell></div> <div class=cell-content><p></p> <details class="info cell"> <summary>Break</summary> <p></p></details></div> </div> <div class=row> <div class=cell><b>10:20 AM</b></div> <div class=cell>Gabriele Bozzola</div> <div class=cell-content><p></p> <details class="abstract cell"> <summary>Beyond Fortran and C++: Exploring Julia for Climate Modeling</summary> <p><strong>[<a href="https://drive.google.com/file/d/1e2QaxNKY2EpvhBwCqT0G0bxyqA5C50cG/view?usp=drive_link">Slides</a>] - [<a href="https://www.youtube.com/watch?v=ErMtewdCY4s&t=6697s">Recording</a>]</strong> </p> <p><em>Gabriele Bozzola is a Senior Software Engineer at CliMA, where we use Julia to build a new GPU, ML-native climate model.</em></p> <p>The Climate Modeling Alliance (CliMA), a collaboration between Caltech, MIT, and JPL, is developing a new climate model using Julia. This talk provides a firsthand account of the experience, highlighting both the advantages and challenges of this choice. We will examine Julia's performance, ease of use for scientists, and its ability to accelerate development cycles. The presentation will also address limitations and opportunities for improvement, offering a balanced assessment of Julia's suitability for various aspects of climate model development and deployment. </p></details></div> </div> <div class=row> <div class=cell><b>10:40 AM</b></div> <div class=cell>Hilario Torres</div> <div class=cell-content><p></p> <details class="abstract cell"> <summary>Dynamic Runtime Scheduling in MPI Applications Via Directed Acyclic Graphs</summary> <p><strong>[<a href="https://drive.google.com/file/d/1c4JyYjXUb1hdL62N6j6WSAN-mLedxk9Z/view?usp=drive_link">Slides</a>] - [<a href="https://www.youtube.com/watch?v=ErMtewdCY4s&t=7953s">Recording</a>]</strong></p> <p><em>Dr. Hilario (Lalo) Torres recently joined the Applications Scalability and Performance (ASAP) group at the National Center for Atmospheric Research (NCAR) as a software engineer. Prior to joining NCAR he was postdoctoral research fellow at NASA Ames Research Center. While at NASA he designed and developed a task based dynamic runtime scheduling system to interface with NASA multi-physics high performance computing (HPC) applications. He received his PhD in mechanical engineering from Stanford University in 2021 where his dissertation focused on writing performance portable multi-physics solvers for heterogeneous HPC systems. His current research interests are at the intersection of computational physics and HPC</em></p> <p>The current state-of-the-practice for Single-Program, Multiple-Data (SPMD) applications utilizes a bulk-synchronous paradigm (BSP) implemented with non-blocking Message Passing Interface (MPI) communication calls. In this paradigm, the order of execution of the computational kernels is hard coded at compile time in order to overlap communication and computation in a synchronized fashion. In simple applications this approach is relatively easy to implement and can provide sufficient parallel scalability. However, it is difficult to specify a performant schedule at compile time for applications that simultaneously run multiple interdependent algorithms on a diverse set of data structures. This presentation covers a library that we have developed, Task Graph Scheduler (TGS), to solve this problem by dynamically scheduling computational kernels at runtime using directed acyclic graphs to track the data dependencies between kernels. This system was specifically designed to leverage existing computational infrastructure as much as possible, supporting the extension of legacy applications. TGS has been incorporated into the eddy high-order multi-physics solver developed at NASA. Details regarding the implementation, our experiences using this system, and performance will be discussed.</p> <p>*This research was supported by an appointment to the NASA Postdoctoral Program at the NASA Ames Research Center, administered by Oak Ridge Associated Universities under contract with NASA. Resources supporting this work were provided by the NASA Transformational Tools and Technologies project Revolutionary Computational Aerosciences program and by the NASA High-End Computing (HEC) Program through the NASA Advanced Supercomputing (NAS) Division at Ames Research Center. </p></details></div> </div> <div class=row> <div class=cell><b>11:00 AM</b></div> <div class=cell>Nat Efrat-Henrici</div> <div class=cell-content><p></p> <details class="abstract cell"> <summary>Advancing Climate Modeling with Collaborative Software Development in Julia</summary> <p><strong>[<a href="https://drive.google.com/file/d/1bajIMgomirP5tJeB63UFAiJq9IoSaWCc/view?usp=drive_link">Slides</a>] - [<a href="https://www.youtube.com/watch?v=ErMtewdCY4s&t=9131s">Recording</a>]</strong></p> <p><em>Nathanael (Nat) Efrat-Henrici is a Software Engineer working at Caltech as part of the Climate Modeling Alliance (CliMA). He works primarily on the atmosphere model and data assimilation infrastructure. Nat obtained his B.S. in Computer Science from Harvey Mudd College in 2022.</em></p> <p>The Climate Modeling Alliance (CliMA) is developing a novel earth system model in Julia with a built-in data assimilation framework. This talk explores lessons learned in empowering research scientists by rethinking the development of scientific software.</p> <p>I will discuss the evolving collaboration between scientists and software engineers, highlighting how we lower barriers to computational research. Central to this approach is our implementation of continuous integration and deployment with robust reproducibility testing, which accelerates iterative development.</p> <p>A key theme of this talk is the importance of abstracting technical complexities while maintaining flexibility and power for advanced research. I will showcase how we have streamlined everyday tasks such as diagnostic output visualization and routine workflow automation. At the same time, we have enabled researchers to easily orchestrate complex, reproducible simulations that scale efficiently across high-performance computing clusters using portable code and data containers. Our model is underpinned by data assimilation methods that simplify the process of model calibration. This allows scientists to integrate diverse real-world observations to account for small-scale, unmodeled processes without requiring deep expertise in machine learning. These insights offer a nuanced perspective on accelerating computational research through thoughtful software design. </p></details></div> </div> <div class=row> <div class=cell><b>11:20 AM</b></div> <div class=cell>Jorge Bravo</div> <div class=cell-content><p></p> <details class="abstract cell"> <summary>Free and open-source tools to develop a web-map visualizer</summary> <p><strong>[<a href="https://drive.google.com/file/d/1dMUy5vnI21QEtHHhLXAGsz5D-zFVU1Iu/view?usp=drive_link">Slides</a>] - [<a href="https://www.youtube.com/watch?v=ErMtewdCY4s&t=10030s">Recording</a>]</strong></p> <p><em>Jorge Humberto Bravo Mendez, with a background in atmospheric sciences and a master‚Äôs degree in hydrometeorology, is currently a PhD candidate at Stevens Institute of Technology (Hoboken, NJ). His research focuses on numerical modeling and remote sensing.</em></p> <p>In this talk, I will discuss how to use various free and open-source tools to visualize geospatial data. While I will showcase a few applications, my primary focus will be on satellite data.</p> <p>When working with data from numerical or satellite models, typically in formats such as GRIB, NetCDF, GeoTIFF, etc., visualization becomes essential. While the initial phase involves visualizing the data using software (e.g., Unidata-IDV, NCview, QGIS) or programming languages (e.g., MATLAB, NCL, Python, R), a website is often required in the next phase to effectively present the results to the public.</p> <p>By leveraging Python, HTML, JavaScript, CSS, Google Earth Engine, and GitLab, it is possible to develop a fully functional website for displaying geospatial data. </p></details></div> </div> <div class=row> <div class=cell><b>11:40 AM</b></div> <div class=cell>Brian Vanderwende</div> <div class=cell-content><p></p> <details class="abstract cell"> <summary>Improving the PBS Pro Experience for NCAR HPC Users</summary> <p><strong>[<a href="https://drive.google.com/file/d/1GDW0LAp6zXPtPySNl0rhOChdAK-fmkfV/view?usp=drive_link">Slides</a>] - [<a href="https://www.youtube.com/watch?v=ErMtewdCY4s&t=10718s">Recording</a>]</strong></p> <p><em>Brian Vanderwende is a HPC User Support Consultant in the Computational Information Systems Laboratory at NSF NCAR in Boulder, CO. He began his career as a PhD student in atmospheric science and leverages that experience to inform his work improving the user experience for researchers. He currently focuses primarily on software stack curation, front-end tool development, and user education.</em></p> <p>For most HPC users, the jobs scheduling software is an integral component of the system, allowing access to the vast compute resources that distinguish a cluster from a workstation. A few workload managers are common in traditional scientific HPC (e.g., Slurm and PBS) with newer tools like Kubernetes also becoming more common. For better or worse, HPC users and administrators are often forced to adapt to a new scheduler upon procurement of the latest system, at which point they come to appreciate the strengths and limitations of the new tool.</p> <p>At NSF NCAR, our two main clusters - Derecho and Casper - both run PBS Pro, though we have used Slurm in the recent past as well. Compared to some of its competitors, PBS Pro's user interface has notable deficiencies: users cannot query historical jobs beyond a few days, administrators cannot query relative job execution priorities, and some queries impose a serious performance impact on the PBS server. To mitigate these weak points, we have developed a number of tools including a cached qstat (job query tool), qhist (a historical record tool), and pbs_prio (a priority query tool).</p> <p>In this talk, we introduce these tools for those unfamiliar, and also discuss recent efforts to modernize them. Such efforts include adding requested features, increasing robustness, incorporating more modern Python programming practices, improving documentation, converting scripts into actual Python packages, and adding regression testing. We will also describe how these improvements to our tools will inform future priorities as we continue to support our PBS Pro users. </p></details></div> </div> <div class=row> <div class=cell><b>12:00 PM</b></div> <div class=cell></div> <div class=cell-content><p></p> <details class="info cell"> <summary>Lunch</summary> <p></p></details></div> </div> <div class=row> <div class=cell><b>1:00 PM</b></div> <div class=cell>Agnieszka ≈ªaba</div> <div class=cell-content><p></p> <details class="abstract cell"> <summary>Continuous Integration with research notebooks: on maintaining reproducibility in atmospheric modeling</summary> <p><strong>[<a href="https://drive.google.com/file/d/1Gf215UR210X6DQjqLQ-upcUAK-60idcf/view?usp=drive_link">Slides</a>] - [<a href="https://www.youtube.com/watch?v=ErMtewdCY4s&t=16375s">Recording</a>]</strong></p> <p><em>Agnieszka is a PhD student in the Environmental Physics Group (zfs.agh.edu.pl/en) at the Faculty of Physics and Applied Computer Science, AGH University of Krakow. My research interests lie at the intersection between cloud microphysics and climate-science applications of water isotopes. I am a member of the github.com/open-atmos community, and the maintainer of PySDM project.</em></p> <p>The maintenance of research-result reproducibility can support rather than be a challenge of ongoing project development. The integration of research notebooks with automated software testing workflows is an essential prerequisite for this. We present reusable tools and solutions engineered in the development and maintenance of the PySDM (open-atmos.github.io/PySDM) and PyMPDATA (open-atmos.github.io/PyMPDATA) atmospheric modeling projects. Both packages are developed entirely in Python, using just-in-time compilation tools (Numba &amp; NVRTC) to enable a single-language HPC tech stack that covers simulation, analysis, and visualization codes. We will discuss the perspectives of both user and developer on reproducibility.</p> <p>From the user's perspective, maintenance of notebooks that reproduce paper results using up-to-date project codebase serves the purpose of documenting and exemplifying project features and applications. It enables exploratory usage with little-to-no effort needed to set up a working environment. However, this is contingent on a design embracing modularity and inversion of control - it is not uncommon in atmospheric modeling for papers to use different simulation flow control or different parameterizations. We present the inversion of control solutions from PySDM that enable the choice of formulae and constants from user code, without trade-offs in: (i) performance, (ii) ability to switch between CPU and GPU backends, and (iii) dimensional analysis of physics-relevant routines for testing unit correctness. The maintenance of notebooks within code repositories also poses challenges in terms of handling embedded visuals. The jupyter-utils project (pypi.io/p/open-atmos-jupyter-utils) helps with embedding GitHub-renderable and Jupyter-book-compatible vector graphics and animations. </p> <p>From the developer's perspective, research notebooks within the code repository are a source of test scenarios and constraints for the assertions that constitute a robust regression-test suite. We present the notebook_vars() function from the jupyter-utils package, designed to be used in concert with the fixture logic of the pytest framework. It enables the execution of the notebook code once in a test session, allowing us of the final notebook state in multiple automated tests. This occurs without modifications to the notebooks themselves. In notebooks pertaining to specific research studies, a direct link from test code up to subject literature is provided. Overall, we achieve an improvement in code readability and refactoring capability.</p> <p>The presented solutions, along with the availability of platforms such as Google Colab, mybinder.org or institutional Jupyter hubs, ensure single-click reproducibility of research-paper results, and a structure for retaining this through future releases of the code base. In addition to the benefits for software users and developers, this satisfies the scientific-method and research-journal reproducibility requirements. </p></details></div> </div> <div class=row> <div class=cell><b>1:20 PM</b></div> <div class=cell>Brett Neuman</div> <div class=cell-content><p></p> <details class="abstract cell"> <summary>GitHub Actions Workflows using Self-Hosted Runners for HPC</summary> <p><strong>[<a href="https://drive.google.com/file/d/1wDHHwr8dQ4fvSxierxZ1wLhX7rqXKpOk/view?usp=drive_link">Slides</a>] - [<a href="https://www.youtube.com/watch?v=ErMtewdCY4s&t=17493s">Recording</a>]</strong></p> <p><em>Brett is a consultant for the Consulting Services Group (CSG) in the Computational and Information Systems Laboratory (CISL) at NCAR. Brett supports the High Performance Computing (HPC) research efforts in distributed computing workflows, GPU porting, and testing for new architectures. His previous research focused on FPGA, accelerator, and mixed precision architectures within Los Alamos National Laboratory‚Äôs (LANL) HPC Design group.</em></p> <p>Github Actions provides tools to implement automated workflows for code but primarily relies on virtual runner environments for their execution. Code designed for High-Performance Computing often requires specific architectures, software stacks, and scheduler information that do not map to the standard virtual runners. To improve code verification and testing automation, we show how combining GitHub Actions workflows with self-hosted runners can be used for code designed to run on HPC systems. The self-hosted runner listens for matching workflow flags and sends them to the job scheduler using parameters defined within the GitHub Actions workflows. </p> <p>While a powerful tool, self-hosted runners present a number of security concerns due to the potential for code injection via unverified commits. We test and demonstrate a number of mitigations developers can employ to reduce these risks. We also provide a case study on how to implement a self-hosted runner on a HPC system and how to create workflows that will use the job scheduler to target different architectural requirements for CPUs, GPUs, and memory. </p></details></div> </div> <div class=row> <div class=cell><b>1:40 PM</b></div> <div class=cell>Kyle Chard</div> <div class=cell-content><p></p> <details class="abstract cell"> <summary>Transforming Scientific Discovery with AI/ML and Globus</summary> <p><strong>[<a href="https://drive.google.com/file/d/1sjVSHz6B4Frnnmq-2gW0KCkTQHkm_HIM/view?usp=drive_link">Slides</a>] - [<a href="https://www.youtube.com/watch?v=ErMtewdCY4s&t=18748s">Recording</a>]</strong></p> <p><em>Kyle Chard is a Research Associate Professor in the Department of Computer Science at the University of Chicago. He also holds a joint appointment at Argonne National Laboratory. He received his Ph.D. in Computer Science from Victoria University of Wellington, New Zealand in 2011.He co-leads the Globus Labs research group, which focuses on a broad range of research problems in data-intensive computing and research data management.</em></p> <p>Scientific instruments produce enormous volumes of data that may exceed local processing capacity. Online analysis combined with AI/ML methods presents one way of dealing with such massive data streams: intelligently collecting only interesting subsets or directing instruments to relevant areas of experimental space. Globus offers a secure, scalable infrastructure for data transfer, management, sharing and connecting instruments, and high-performance computing and storage systems. This talk will explore how Globus is helping researchers automate data collection, run distributed computing pipelines, and enable efficient data analysis, AI model training, and simulations. </p></details></div> </div> <div class=row> <div class=cell><b>2:00 PM</b></div> <div class=cell>Maxine Hartnett</div> <div class=cell-content><p></p> <details class="abstract cell"> <summary>Event Driven Architecture in the cloud for the IMAP Science Data Center</summary> <p><strong>[<a href="https://drive.google.com/file/d/15h-2h6SmvKcv_6w1KJpJxP4BmoWGZqNa/view?usp=drive_link">Slides</a>] - [<a href="https://www.youtube.com/watch?v=ErMtewdCY4s&t=19882s">Recording</a>]</strong></p> <p><em>Maxine is a software engineer at LASP in Boulder, Colorado. She is currently the science data center lead for two instruments on IMAP and has contributed to a variety of science data systems for heliophysics and Earth science space missions.</em></p> <p>As cloud based processing becomes more common in the scientific sphere, a huge variety of new tools and techniques are emerging for mission data pipelines. Upcoming missions are getting a new opportunity to explore these techniques from the ground-up. IMAP, a heliophysics mission launching in 2025, is able to develop an entire pipeline with a cloud-first attitude. The IMAP science data center, based out of LASP, uses tools such as AWS, infrastructure as code, and docker to create a flexible, reliable, and efficient event-based processing pipeline. </p> <p>The IMAP mission has 10 instruments, all of which are interdependent, which cover a broad scope of scientific data. By creating an event-based system in the cloud, the SDC can extend and modify processing based on changing requirements, while also ensuring that processing occurs quickly and reliably. This science data system takes advantage of the cloud's new ecosystem to create a pipeline that runs only what is needed, when it's needed, using small, distinct pieces of code that are easy to maintain and modify by the entire team. As the cloud becomes more wildely used, it is time to rethink the way we create processing pipelines so we can take advantage of the powerful opportunities provided by AWS and other cloud providers. </p></details></div> </div> <div class=row> <div class=cell><b>2:20 PM</b></div> <div class=cell></div> <div class=cell-content><p></p> <details class="info cell"> <summary>Break</summary> <p></p></details></div> </div> <div class=row> <div class=cell><b>2:50 PM</b></div> <div class=cell>Isaac Schluesche</div> <div class=cell-content><p></p> <details class="abstract cell"> <summary>Using Julia for Next-Generation Atmospheric Analysis Software</summary> <p><strong>[<a href="https://drive.google.com/file/d/10XA-VbfeTjmvFXMRN654GNCXtdn1Yi9P/view?usp=drive_link">Slides</a>] - [<a href="https://www.youtube.com/watch?v=ErMtewdCY4s&t=22942s">Recording</a>]</strong></p> <p><strong>Coauthors: Dr. Michael M Bell</strong></p> <p><em>Isaac is a second year master's student at Colorado State University working on developing machine learning algorithms to remove non-meteorological data from radar sweeps using Julia.</em></p> <p>Python and Matlab have been widely adopted as languages of choice in geoscience education due to their high-level interface and easy to use syntax, but their interpreted nature generally makes them slower than low-level languages without extensive optimizations or porting performance-critical portions to C. In the atmospheric sciences, this trade off has commonly resulted in computationally intensive codebases such as numerical weather prediction (NWP) and data assimilation being written partially or wholly in low-level languages (e.g., C, Fortran) for efficiency, which can be more difficult to actively develop and modify than higher-level languages. Julia is a relatively new language designed for scientific computing that combines a high-level and intuitive syntax similar to Python and Matlab with the inherent performance of lower-level languages in part through ‚Äújust-in-time‚Äù compilation, making it natively as fast as C or Fortran in several benchmarks. Julia is based on a ‚Äúmultiple-dispatch‚Äù design philosophy that is well suited to many scientific applications. This talk will detail efforts being made at Colorado State University to implement performant and readable code using Julia across a wide variety of meteorological applications ranging from radar processing to idealized numerical weather prediction (NWP) experiments.</p> <p>Radar quality control (QC) involves removing non-meteorological echoes such as the earth‚Äôs surface or biological targets and is important for obtaining useful information from radar data. In the past, data was QCed by radar meteorologists parsing the data by hand or using relatively simple rules-based thresholding methods. Recently, a Machine Learning (ML) technique was created in Python that outperformed the existing methods. A new version of the automated removal algorithm, named RONIN.jl (Random-forest Optimized Nonmeteorological IdentificatioN) has been developed entirely in Julia that further increases the accuracy of the QC, while operating at speeds orders of magnitude quicker than the original Python code - highlighting the utility of Julia for ML applications. </p> <p>A NWP model aimed at investigating atmospheric dynamics, coined Scythe.jl, has also been created in Julia and will be presented. Scythe is based on the spectral transform method and uses a mixture of cubic B-spline, Fourier, and Chebyshev basis functions to represent physical variables and their spatial derivatives in a variety of coordinate systems, with multi-processing capabilities using both the Julia Distributed package and multi-threading.. The spectral gridding engine of Scythe.jl, dubbed Springsteel.jl, is also being utilized as the basis for a variational data analysis and assimilation tool called Daisho.jl. The combination of these Julia packages will ultimately provide an end-to-end analysis, assimilation, and simulation toolkit that interfaces seamlessly with the Lidar Radar Open Software Environment (LROSE) for next-generation processing of remote sensing data from the Airborne Phased Array Radar (APAR) for improved understanding and prediction of high-impact weather. </p></details></div> </div> <div class=row> <div class=cell><b>3:10 PM</b></div> <div class=cell>Jen DeHart</div> <div class=cell-content><p></p> <details class="abstract cell"> <summary>Improved Accessibility and Community Knowledge of Lidar and Radar Data Analysis</summary> <p><strong>[<a href="https://drive.google.com/file/d/1CxH_qufKStz3i8kkY9I5SCbYJ3Os2nK9/view?usp=drive_link">Slides</a>] - [<a href="https://www.youtube.com/watch?v=ErMtewdCY4s&t=23779s">Recording</a>]</strong></p> <p><strong>Coauthors: Ana Espinoza</strong></p> <p><em>Jen is a Research Scientist II at Colorado State University. In her research, Jen uses a combination of polarimetric radar observations and mesoscale models to understand the processes responsible for heavy tropical rainfall in a variety of weather systems. Jen is also the CSU PI of the NSF-funded LROSE project, which develops open-source software tools for working with radar and lidar data.</em></p> <p>To improve accessibility and community knowledge of applications in the Lidar Radar Open Software Environment (LROSE), a team from the National Science Foundation (NSF) National Center for Atmospheric Research, Colorado State University, and NSF Unidata has developed a lidar and radar meteorology science gateway deployed on the NSF Jetstream2 cloud. Utilizing the ‚ÄúZero to JupyterHub with Kubernetes‚Äù workflow, the science gateway integrates LROSE with other lidar and radar meteorology software packages. This integration allows users to execute applications directly from the JupyterLab terminal, streamlining the creation of datasets for further analysis and visualization within Jupyter notebooks. By combining traditional command-line operations with modern Python-based tools for data analysis and visualization, this gateway provides a robust end-to-end solution that caters to both educational and research needs. The gateway has already facilitated LROSE instructional workshops and classroom exercises. Our work demonstrates the significant potential of merging established scientific computing techniques with advanced Python environments, opening new avenues for computational science education and research.</p> <p>The LROSE team has acquired successive allocations on the NSF Jetstream2 cloud at Indiana University through ACCESS. To develop the LROSE Science Gateway, we employed the ‚ÄúZero to JupyterHub with Kubernetes‚Äù workflow ported to the NSF Jetstream2 cloud, enabling rapid and scalable deployment to accommodate a variable number of users. Authentication is managed through either GitHub OAuth or temporary credentials, depending on the situation. Since LROSE is a collection of C/C++ applications, we configured Docker containers based on the Jupyter Docker Stack to integrate the LROSE software, available via the JupyterLab terminal. These containers also include Conda package manager environments equipped with Python packages like Py-ART, CSU RadarTools, and Metpy for further data analysis. A shared drive accessible to all participants contains instructional datasets for lidar and radar data analysis.</p> <p>Tutorials take the form of Jupyter notebooks for use by individuals, in classroom exercises, or at instructional workshops. Some tutorials are complete with pre-loaded examples to quickly visualize workflows and results. Other tutorials guide students how to run the applications independently. All tutorials are hosted on the LROSE Science Gateway GitHub repository, which is open to contributions from colleagues and community members.</p> <p>Future plans include an "intermediate" level workshop on SAMURAI, one of the multi-Doppler wind applications of the LROSE suite. Additionally, work is currently underway to run GUI applications in the same browser-based JupyterLab environment. GUI applications for radar and lidar data visualization utilize the QT framework and present unique technical challenges. The techniques to accomplish GUI access have immediate applications for other GUI programs, such as NFS Unidata's IDV and their version of the AWIPS CAVE data visualization tools. Lastly, as demand for the resources found on the gateway increases, it becomes increasingly important to efficiently manage the Jetstream2 resources allocated by the ACCESS program. LROSE, NSF Unidata, San Diego Supercomputing Center (SDSC), and Indiana University staff are working together to deploy and evaluate Kubernetes cluster auto-scaling. With autoscaling, resources will no longer sit idle while awaiting new logins and will instead be provisioned on-demand. </p></details></div> </div> <div class=row> <div class=cell><b>3:30 PM</b></div> <div class=cell>Amit Ruhela</div> <div class=cell-content><p></p> <details class="abstract cell"> <summary>An introduction to TACC HPCPerfStats</summary> <p><strong>[<a href="https://drive.google.com/file/d/1jkuOaAC0HDLEUqGetKl-yL798205z97Q/view?usp=drive_link">Slides</a>] - [<a href="https://www.youtube.com/watch?v=ErMtewdCY4s&t=24978s">Recording</a>]</strong></p> <p><strong>Coauthors: John Cazes</strong></p> <p><em>Dr. Amit Ruhela works as a Manager in the HPC group at TACC, Austin. He earned a Ph.D. in Computer Science from IIT Delhi and postdoc experience from The Ohio State University. Dr. Ruhela conducts research and engineering in Communication interconnects, Parallel Computing, Big Data, and Machine Learning domains. He is primarily focused on feature and performance optimizations in MPI communication through novel and innovative designs and serves as PI of Rockport Networks Center of Excellence at TACC.</em></p> <p>HPCPerfSTATS is a comprehensive infrastructure designed for the low-overhead collection and analysis of system-wide performance data, integrating information from diverse sources. It provides a web-based interface that enables users to explore job-specific and system-level reports, perform automated analyses, and identify jobs requiring human intervention.</p> <p>The HPCPerfSTATS monitor operates periodically during job execution, collecting extensive system statistics and hardware performance counter data. This includes CPU usage, socket-level memory utilization, swapping and paging statistics, system load and process metrics, block device and system counters, interconnect fabric traffic, filesystem usage (e.g., NFS, Lustre, Panasas), and detailed CPU and Uncore performance metrics (e.g., Memory Controller, Cache, NUMA Coherence Agents, Power Control Unit).</p> <p>The accompanying web interface facilitates intuitive navigation of cluster-wide job data, visualization of flagged jobs, and plotting of key job characteristics, offering a powerful toolset for performance monitoring and optimization. </p></details></div> </div> <div class=row> <div class=cell><b>3:50 PM</b></div> <div class=cell>Namita Shah</div> <div class=cell-content><p></p> <details class="abstract cell"> <summary>From Carpentries to Curriculum: Benefits and Drawbacks of Adapting Open Educational Materials to Meet Local Needs</summary> <p><strong>[<a href="https://drive.google.com/file/d/11n3x8DYtUdoyd6uqO-Hx8AaS610hli4F/view?usp=drive_link">Slides</a>] - [<a href="https://www.youtube.com/watch?v=ErMtewdCY4s&t=26333s">Recording</a>]</strong></p> <p><em>Namita Shah is an undergraduate senior at Arizona State University, majoring in Computer Science with a minor in Data Science. Since 2023, she has been working as a Research Software Engineer(RSE) at the Digital Innovation Group under the mentorship of Nicole Brewer and Dr. Julia Damerow. She has also gained experience as an RSE at Princeton over the last summer. Currently, Namita is developing curriculum in collaboration with Nicole to build a "Foundations of Research Software Engineering" course aimed at equipping graduate research students with essential skills in software development. With a passion for both research and education, Namita is committed to RSE endeavors and its role in academic settings!</em></p> <p>Scientific software development thrives on knowledge-sharing, and tutorials have long been a cornerstone of that process whether for training, documentation, or curriculum development. One approach to accelerate development of such materials is to pull content from existing open source or creative commons tutorials, without starting from scratch. Another possible benefit of adapting existing tutorials, such as The Carpentries, is to incorporate the pedagogical expertise already embedded within these resources. Yet, while there is a wealth of existing tutorial content both in open source and in research computing, remixing and adapting tutorial content is surprisingly challenging. As the landscape of scientific computing evolves, the ability to efficiently repurpose and structure existing materials is critical to keeping pace with emerging technologies and best practices. Drawing from our experience developing a notebook-based, semester-long course for graduate researchers, we will discuss practical strategies for curriculum design, the adaptation of existing resources while also considering the broader social challenges of recruitment and time management. This talk explores how educators and research software engineers (RSEs) can effectively tailor existing materials to create structured, impactful learning experiences. </p> <p>The demand for RSEs continues to grow as scientific research becomes increasingly computational. Yet, RSEs come from diverse backgrounds including STEM, the social sciences and humanities, and computing, often acquiring essential software engineering skills informally or on the job. This hodgepodge of experience presents challenges not only for individuals looking to round out their interdisciplinary skill set but also for teams striving for maintainable, reproducible, and efficient software development in research-oriented settings. Recognizing this gap from our own RSE experiences at interdisciplinary labs, we set out to develop a semester-long curriculum designed to equip non-CS graduate students with the software engineering skills necessary for research environments. Our approach involved leveraging existing Software Carpentries modules and structuring them into a ‚ÄúFoundations of Research Software Engineering‚Äù lab course. Throughout this process, we encountered certain challenges in adapting these resources - how to effectively pull relevant parts of the lesson, how to make changes without losing pedagogical integrity, and how to balance core concepts with real-world applications. In this talk, we share our experience navigating tutorial adaptation and our takeaways from that experience, highlighting both the benefits and drawbacks of this process, as well as the social elements required to execute the human side of curriculum development such as recruiting students and time management in planning. We will also introduce the concept of Open Source Education (OSE) and review the legality of remixing open source or creative commons tutorials.</p> <p>These takeaways can be applied to other use cases where educators, researchers, and developers may feel overwhelmed by the vast amount of existing material and struggle to tailor it to their specific needs. </p></details></div> </div> <div class=row> <div class=cell><b>4:10 PM</b></div> <div class=cell></div> <div class=cell-content><p></p> <details class="success cell"> <summary>Notebook Proceedings Office Hours</summary> <p>During the Notebook Proceedings Office Hours, we will provide guidance on preparing and formatting Jupyter Notebook-based conference proceedings. Attendees can get help with markdown formatting, reproducibility best practices, code execution issues, and submission requirements to ensure their notebooks meet the conference standards.</p> </details> <p></p></div> </div> <div class=row> <div class=cell><b>5:00 PM</b></div> <div class=cell></div> <div class=cell-content><p></p> <details class="warning cell"> <summary>End of Day 1</summary> <p></p></details></div> </div><p></p> </div> </div> <div class=tabbed-block> <p></p><div class=table> <div class=row> <div class=cell><b>8:30 AM</b></div> <div class=cell></div> <div class=cell-content><p></p> <details class="info cell"> <summary>Opening Remarks</summary> <p></p></details></div> </div> <div class=row> <div class=cell><b>8:40 AM</b></div> <div class=cell>Romit Maulik</div> <div class=cell-content><p></p> <details class="abstract cell"> <summary>Invited Keynote: Automated deep ensemble with uncertainty quantification using DeepHyper</summary> <p><strong>[<a href="https://drive.google.com/file/d/1FAWuYB43uLdDocRfA01aGCaT75OqkOju/view?usp=drive_link">Slides</a>] - [<a href="https://www.youtube.com/watch?v=FwJNPHx86GY&t=1244s">Recording</a>]</strong></p> <p><em>Dr. Romit Maulik is an Assistant Professor in the College of Information Sciences and Technology at Pennsylvania State University (Penn State). He is also a co-hire in the Institute for Computational and Data Sciences at Penn State and a Joint Appointment Faculty at Argonne National Laboratory. He obtained his PhD in Mechanical and Aerospace Engineering at Oklahoma State University (in 2019) and was the Margaret Butler Postdoctoral Fellow (from 2019-2021) before becoming an Assistant Computational Scientist at Argonne National Laboratory (from 2021-2023). His group studies high-performance multifidelity scientific machine learning algorithm development with applications to various multiphysical nonlinear dynamical systems such as those that arise in fluid dynamics, weather and climate modeling, nuclear fusion, and beyond. He is an Early Career Awardee from the Army Research Office.</em></p> <p>Deep neural networks are powerful predictors for a variety of tasks. However, they do not capture uncertainty directly. Using neural network ensembles to quantify uncertainty is competitive with approaches based on Bayesian neural networks while benefiting from better computational scalability. However, building ensembles of neural networks is a challenging task because, in addition to choosing the right neural architecture or hyperparameters for each member of the ensemble, there is an added cost of training each model. To address this issue, we propose AutoDEUQ, an automated approach for generating an ensemble of deep neural networks. Our approach leverages joint neural architecture and hyperparameter search to generate ensembles. This ensemble and the law of total variance can be used to decompose the predictive variance of deep ensembles into aleatoric (data) and epistemic (model) uncertainties.</p> <p>AutoDEUQ is built into DeepHyper, a scalable python-based library for automated neural architecture and hyperparameter search and can readily be deployed in serial and parallel platforms for deep ensembles based uncertainty quantification. An introductory tutorial to using DeepHyper and its ensembles based UQ capabilities is available here:<br> <a href=https://deephyper.readthedocs.io/en/latest/examples/examples_uq/plot_nas_deep_ensemble_uq_regression_pytorch.html#sphx-glr-examples-examples-uq-plot-nas-deep-ensemble-uq-regression-pytorch-py>https://deephyper.readthedocs.io/en/latest/examples/examples_uq/plot_nas_deep_ensemble_uq_regression_pytorch.html#sphx-glr-examples-examples-uq-plot-nas-deep-ensemble-uq-regression-pytorch-py</a> </p></details></div> </div> <div class=row> <div class=cell><b>9:50 AM</b></div> <div class=cell></div> <div class=cell-content><p></p> <details class="info cell"> <summary>Break</summary> <p></p></details></div> </div> <div class=row> <div class=cell><b>10:20 AM</b></div> <div class=cell>Ana Manica</div> <div class=cell-content><p></p> <details class="abstract cell"> <summary>Software in Space Science: Developing Open Source Metadata Management</summary> <p><strong>[<a href="https://drive.google.com/file/d/1Yv0yOLJmqpWTl32M28CxIFySuCkEIy7k/view?usp=drive_link">Slides</a>] - [<a href="https://www.youtube.com/watch?v=FwJNPHx86GY&t=7576s">Recording</a>]</strong></p> <p><em>Ana Manica is a current undergrad at the University of Colorado Boulder pursuing a dual degree in Computer Science and Astrophysics.</em></p> <p>As more data is collected and utilized by scientists across the globe, the categorization, organization, and display of data has become increasingly important. The Interstellar Mapping and Acceleration Probe mission (IMAP) strives to create elegant metadata management software to be employed not only across this mission, but others as well.</p> <p>This talk will begin with a brief overview of the different forms of metadata and file formats that are required by scientists and space agencies to properly display and understand information collected by spacecraft. I will discuss some of the difficulties and design choices that went into the current solution. Finally, this talk will cover the open source nature of this software with the creation of SAMMI and collaboration with NASA's HERMES mission. As a student developer working at LASP with the IMAP SDC, this talk will expand upon my experiences, my particular work with this mission, and the importance of open source software. </p></details></div> </div> <div class=row> <div class=cell><b>10:40 AM</b></div> <div class=cell>Jiachen Liu</div> <div class=cell-content><p></p> <details class="abstract cell"> <summary>Implementation of the Hyperdual-Step Method in CMAQ for Numerically Exact Sensitivity Analysis</summary> <p><strong>[<a href="https://drive.google.com/file/d/1jgZWTq0sUvfLB-LsGNaq0CC1AcRYOjfg/view?usp=drive_link">Slides</a>] - [<a href="https://www.youtube.com/watch?v=FwJNPHx86GY&t=8358s">Recording</a>]</strong></p> <p><em>Jiachen Liu is a Ph.D. candidate at Drexel University. He is interested in developing and applying numerical techniques to analyze sensitivities of complex chemical transport models</em></p> <p>Sensitivity analysis in chemical transport models quantifies the response of output variables to changes in input parameters, providing valuable information for model development, data assimilation, and air pollution control strategy design. Traditional sensitivity analysis methods, such as the finite-difference method, the direct decoupled method (DDM), the complex variable method, and the adjoint method, have limitations. Some suffer from numerical errors when applied to nonlinear models (e.g., finite difference and complex step methods), while others (e.g., DDM and adjoint methods) require significant effort to maintain when the base model is updated.</p> <p>To address these challenges, we present CMAQ-hyd, an augmented version of the Community Multiscale Air Quality model (CMAQ), implementing the hyperdual-step method for computing numerically exact first- and second-order sensitivities of species concentrations with respect to emissions and initial conditions. Compared to CMAQ-DDM and CMAQ-adjoint, CMAQ-hyd is easier to update and maintain while remaining free of subtractive cancellation and truncation errors. Furthermore, it achieves these improvements while being computationally efficient, reducing the resource burden compared to traditional finite-difference methods for the same sensitivity calculations.</p> <p>We will also showcase the implementation of the hyperdual-step method in other potential applications using an automated file conversion process. This method can be seamlessly integrated into various models where accurate first- and second-order sensitivity calculations are essential for research and analysis. </p></details></div> </div> <div class=row> <div class=cell><b>11:00 AM</b></div> <div class=cell>Prentice Bisbal</div> <div class=cell-content><p></p> <details class="abstract cell"> <summary>The first step to making your code more accessible: Make it easier to build and install!</summary> <p><strong>[<a href="https://drive.google.com/file/d/1UXJjfc_DWsCCmmziLpDMzriyfWfX1jiO/view?usp=drive_link">Slides</a>] - [<a href="https://www.youtube.com/watch?v=FwJNPHx86GY&t=9653s">Recording</a>]</strong></p> <p><em>Prentice Bisbal is an HPC Systems Engineer III in the Computational and Information Systems Lab (CISL) at NSF NCAR. He has over 20 years of experience as a Linux system administrator specializing in high performance computing. Throughout most of his career he has been responsible for managing scientific software and has probably gone through the configure/build/install process for open-source software more times than he can remember.</em></p> <p>One of the goals of publicly-funded software development is to make the results of that software development accessible to the public for use by the public. This means that the code must be downloadable and buildable by the public. Unfortunately, scientific software is usually much harder to download and compile than general-purpose software, presenting difficulties for even the most experienced scientific computing software managers.</p> <p>In this talk, the author will explain why making your code easier to build and install can lead to a virtuous circle that increases the overall success of your project. This talk will include some of the difficulties the author has experienced installing scientific software and contrast this with the build process(es) used by more general purpose software. Suggestions on how to improve the build process will be provided. </p> <p>Finally, current trends in open-source software distribution, such as distributing code through GitHub and distributing executables in containers and the weaknesses of those approaches, will be discussed. </p></details></div> </div> <div class=row> <div class=cell><b>11:20 AM</b></div> <div class=cell>David Pettifor</div> <div class=cell-content><p></p> <details class="abstract cell"> <summary>Designing a spectrum visualization platform with flexibility in mind</summary> <p><strong>[<a href="https://drive.google.com/file/d/1Nw5PDP7GrJBh98v-zWZXSj2Xk8W3on3_/view?usp=drive_link">Slides</a>] - [<a href="https://www.youtube.com/watch?v=FwJNPHx86GY&t=11240s">Recording</a>]</strong></p> <p><strong>Coauthor: Melissa Harden</strong></p> <p>*David Pettifor, Research Software Engineer Manager, has been working for the Center for Research Computing at the University of Notre Dame for 14 years. His background is in software development, specifically database driven web portals for research support spanning multiple domains. The goal of any project is to foster research, exploration, and understanding of data through intuitive web portal interfaces.</p> <p>Melissa Harden, Senior Product Owner, has been with thee Center for Research Computing at the University of Notre Dame for three years. In this role, she focuses on collaborator goals and priorities for their research projects and ensures the delivery of valuable research software.*</p> <p>Effective visualization is critical for diverse research communities, particularly when dealing with complex datasets. The Center for Research Computing (CRC) at the University of Notre Dame is developing an innovative visualization platform for radio frequency (RF) spectrum data as part of SpectrumX's Flagship Project 1, which aims to advance sensing and understanding of spectrum coexistence (<a href=https://www.spectrumx.org/project/spectrum-awareness-for-coexistence/ >https://www.spectrumx.org/project/spectrum-awareness-for-coexistence/</a>). Our visualization platform is designed to facilitate exploration and analysis of radio frequency data, catering to a broad audience that includes experienced researchers, novice analysts, and policymakers. To achieve our goal of flexibility, we have focused on creating a user-centric interface that allows for multiple methods of data access‚Äîincluding private uploads and integration with publicly available datasets stored in a spectrum data platform, also in development at the CRC.</p> <p>Key features of the platform include visualization creation wizards tailored to common visualization types in radio frequency research. These wizards guide users through a step-by-step process, ensuring that all necessary inputs for effective visualizations are considered. Additionally, we will integrate Jupyter notebooks as an embedded Python environment to enable users to enhance and customize base visualizations and dataset aggregations, further accommodating both novice and experienced users. Educational resources, including tutorials, will also be available to facilitate user engagement and skill development. Throughout the presentation, we will intersperse reflections on our roles as Research Software Engineer Manager/Technical Lead and Product Owner, highlighting how our agile software development approach has shaped the identification and prioritization of key features and components. The Technical Lead offers direction on technical strategy and implementation, while the Product Owner ensures alignment with overall project objectives and user needs.</p> <p>This presentation will not only share our insights into software development best practices but also illustrate our commitment to building a platform that meets the diverse needs of users in the realm of spectrum analysis. </p></details></div> </div> <div class=row> <div class=cell><b>11:40 AM</b></div> <div class=cell>Tori Marbois</div> <div class=cell-content><p></p> <details class="abstract cell"> <summary>Modernizing Legacy Systems: Updating the MAVEN Science Data Center for Improved Performance and Maintainability</summary> <p><strong>[<a href="https://drive.google.com/file/d/1seNo5TvnKDa4PhlY03UqcJ2Gobif9Txh/view?usp=drive_link">Slides</a>] - [<a href="https://www.youtube.com/watch?v=FwJNPHx86GY&t=12481s">Recording</a>]</strong></p> <p><em>Tori Marbois (she/her) works at LASP as a Data Systems Engineer and recently joined the MAVEN Science Data Center (SDC) team to support ongoing operations and modernization efforts. She has a degree in Computational and Applied Mathematics from the Colorado School of Mines and has lived in Colorado since 2017.</em></p> <p>Many organizations face the dilemma of maintaining legacy systems, which can rely on deprecated components, include difficult-to-read code, or under-perform as requirements and standards evolve. Major updates are often deprioritized due to the non-trivial effort required to complete them and in favor of competing demands for resources on new projects. There's also a hesitation to modify functioning systems, fearing disruptions to service. However, taking the time to modernize a system that continues to serve a valuable purpose has many benefits. Last year, the MAVEN Science Data Center (SDC) at the Laboratory for Atmospheric &amp; Space Physics (LASP) began efforts to bring its pipeline to current standards and best practices with the goal of creating an improved workflow that is easier to maintain.</p> <p>The MAVEN SDC hosts the data collected by the Mars Atmosphere and Volatile Evolution (MAVEN) mission, which explores the planet‚Äôs upper atmosphere, ionosphere, and interactions with the sun and solar wind. The SDC maintains a website where users can find instrument documentation, view quicklook plots, and access the data via RESTful APIs. Additionally, we also deliver data quarterly to NASA's Planetary Data Systems archives for long term stewardship. The underlying architecture and code that supports these responsibilities was created over a decade ago, resulting in a brittle codebase and system that are challenging to test and troubleshoot.</p> <p>Our team has improved our workflow by updating the running Python version, replacing a Postgres testing database with a serverless SQLite database, transitioning to open source, refactoring testing frameworks, leveraging GitHub features, and revamping documentation. Further improvement efforts will involve moving the internal storage archive from a deprecated AWS vault to high-performing s3 storage and containerizing the SDC with Docker. These updates have enhanced our CI/CD pipeline and enabled local testing for efficient feature development. The long-term goal is ensuring consistency with other mission SDCs managed by LASP and streamlining the onboarding of new maintainers. This initiative also offers an opportunity to assess new tools and best practices, paving the way for a more robust, accessible and maintainable system. This presentation will share our approach, key insights, and lessons learned on this effort. </p></details></div> </div> <div class=row> <div class=cell><b>12:00 PM</b></div> <div class=cell></div> <div class=cell-content><p></p> <details class="info cell"> <summary>Lunch</summary> <p></p></details></div> </div> <div class=row> <div class=cell><b>1:00 PM</b></div> <div class=cell>Katherine Rasmussen</div> <div class=cell-content><p></p> <details class="note cell"> <summary>Julienne + Assert == Correctness-Checking for Functional Fortran</summary> <p><strong>[<a href="https://drive.google.com/file/d/1FuMk8l81jnIk6jf1oEATpMg-Gq_XYd25/view?usp=drive_link">Slides</a>] - [<a href="https://www.youtube.com/watch?v=FwJNPHx86GY&t=17401s">Recording</a>]</strong></p> <p><strong>Coauthors: Damian Rouson, Dan Bonachea</strong></p> <p><em>Katherine Rasmussen is a Computer Systems Engineer who applies Linguistics knowledge to developing, testing, and compiling programming languages for high-performance computing. She has experience in language grammars and abstract syntax trees (ASTs) for Fortran, C, and C++. She works in the Computer Languages and Systems Software (CLaSS) Group at Lawrence Berkeley National Laboratory where she contributes to the Julienne unit-testing and string-handling utility, the LLVM Flang Fortran compiler, and the Caffeine parallel runtime library. She does software archaeology, digging through ancient layers of legacy code for purposes of modernization, porting, building, testing, and parallelization. She also serves as an alternate on the Fortran Standards committee, has experience organizing the Fortran Standards committee meetings and is the co-Publication chair for CARLA2025, the Latin America High Performance Conference.</em></p> <p>The agile software development practice of test-driven development (TDD) advocates unit testing as an essential driver of software design and construction. In TDD, tests of individual units of software (e.g., procedures) serve documentation and verification roles. As documentation, tests specify the behaviors required for code correctness. Executing a suite of tests verifies that the actual behaviors satisfy the documented requirements. As inspired by the Veggies and Garden unit testing frameworks for modern Fortran, the more lightweight Julienne framework uses the Template Method pattern to report serial or parallel test results in the form of a specification (<a href=https://go.lbl.gov/julienne>https://go.lbl.gov/julienne</a>). As such, Julienne‚Äôs test output names the test subject (e.g., a class or type-bound procedure), the expected behavior, the test outcome (pass or fail), and provides diagnostic information if a test fails.</p> <p>The use of Julienne centers around users defining a test in the form of a non-abstract child type that extends Julienne‚Äôs abstract test_t derived type. The user‚Äôs child type thus inherits an obligation to define type-bound procedures that name the subject of the test and provide the test results. As a template method, test_t‚Äôs type-bound ‚Äúreport‚Äù procedure invokes the user‚Äôs procedures by referencing the aforementioned deferred bindings and reporting on the collective success or failure across multiple images (processes) in programs that use Fortran‚Äôs multi-image parallel programming features.</p> <p>Working from the example test suite in the Julienne repository, attendees will learn how to write and run a simple test suite, including how to use Julienne‚Äôs string-handling for producing rich diagnostic information from a failing test. Attendees will also see examples of Julienne‚Äôs use in other Berkeley Lab software projects such as the Fiats deep learning library and Matcha T-cell motility simulator.</p> <p>Attendees will also learn a functional programming pattern developed and used by the Berkeley Lab Fortran presenters. Functional programming centers around the definition of pure procedures that are free of side effects, including file input and output. To supplement the material on external verification via unit tests, this tutorial will also introduce our Assert utility library and Assert‚Äôs use for runtime correctness-checking inside procedures (<a href=https://go.lbl.gov/assert>https://go.lbl.gov/assert</a>). Attendees will learn how Assert addresses a common reason developers cite for not writing pure procedures: a desire to produce diagnostic output when debugging code. We posit that most developers seek output to verify an expectation about data and that such expectations can be stated in assertions that take the form of logical expressions. Attendees will learn how Assert empowers developers to obtain rich, customized diagnostic information through character stop codes when an assertion fails, resulting in error termination. Attendees will also learn how to use Assert in such a way that guarantees zero runtime overhead by automatically eliminating assertions in production builds of user software. </p></details></div> </div> <div class=row> <div class=cell><b>2:30 PM</b></div> <div class=cell></div> <div class=cell-content><p></p> <details class="info cell"> <summary>Break</summary> <p></p></details></div> </div> <div class=row> <div class=cell><b>3:00 PM</b></div> <div class=cell>Guoqing Ge</div> <div class=cell-content><p></p> <details class="abstract cell"> <summary>Promoting open science: a better tool for version controlling mega binary data files - git-mega</summary> <p><strong>[<a href="https://drive.google.com/file/d/1H9TRQHYoYpy4Fqtu_EvoWFjxjccBHX32/view?usp=drive_link">Slides</a>] - [<a href="https://www.youtube.com/watch?v=FwJNPHx86GY&t=24511s">Recording</a>]</strong></p> <p><em>Dr. Ge is a research scientist at CIRES/NOAA GSL and works on improving the NOAA operational forecasts through the data assimilation developments.</em></p> <p>In recent years, the drive towards open science has been gaining momentum, with researchers recognizing the importance of transparent and accessible data sharing. However, version controlling large binary data files has remained a challenging hurdle in achieving comprehensive open science practices. Existing solutions such as Git LFS have provided some relief, but they struggle to handle mega binary data (a few hundred gigabytes to a few terabytes) with problems such as network traffic jams, long waiting times to clone repositories, consuming a large amount of disk space with multiple copies, costing too much money for hosting data. This talk introduces "git-mega," a cutting-edge tool specifically developed to meet the needs of version-controlling mega binary data files. </p> <p>By capitalizing on the latest advancements in data storage and retrieval, git-mega efficiently manages large binary files within Git repositories, ensuring they remain lightweight and effortlessly accessible. Leveraging unique algorithms, git-mega minimizes the impact of mega binary files on version control operations, enhancing repository cloning speed and responsiveness.</p> <p>Key features of git-mega include intelligent data deduplication, seamless data synchronization with existing storage systems, and comprehensive metadata management. Through its user-friendly interface, researchers can effortlessly integrate git-mega into their existing workflows, eliminating the complexities often associated with handling mega binary data files.</p> <p>Moreover, git-mega adheres to the FAIR (Findable, Accessible, Interoperable, and Reusable) principles, fostering a culture of data sharing and collaboration within the scientific community. Researchers can confidently share their binary data files, knowing that git-mega facilitates effortless discovery, accessibility, and reproducibility of scientific findings.</p> <p>This talk discusses the architecture, implementation, and performance evaluation of git-mega, comparing it with existing solutions to showcase its superiority in managing mega binary data files. By promoting transparent data sharing and fostering collaboration, git-mega brings researchers one step closer to realizing the full potential of open and reproducible scientific endeavors. </p></details></div> </div> <div class=row> <div class=cell><b>3:20 PM</b></div> <div class=cell>Saurav Dey Shuvo</div> <div class=cell-content><p></p> <details class="abstract cell"> <summary>Automating ASOS Data Processing: A Python-Based Solution for Efficient Meteorological Analysis</summary> <p><strong>[<a href="https://drive.google.com/file/d/1mi3I090fITJIxnYHNh4BBZfPachywCLv/view?usp=drive_link">Slides</a>] - [<a href="https://www.youtube.com/watch?v=FwJNPHx86GY&t=25566s">Recording</a>]</strong></p> <p><em>Saurav's research focuses on the use of Numerical Weather Prediction (NWP) as well as Satellite Data. Currently, he is doing Ph.D. at The Ohio State University, under the supervision of Professor Dr. David H. Bromwich. He is positioned at the famous Byrd Polar and Climate Research Center in Columbus, Ohio. The focus of his work on implementing the PIEKTUK model for simulating the blowing snow events.</em></p> <p>The Automated Surface Observing System (ASOS) provides essential meteorological data for weather forecasting, climate studies, and atmospheric research. However, working with raw ASOS data presents significant challenges due to its large volume, inconsistent formatting, and the presence of duplicate or missing entries. Before meaningful analysis can be conducted, extensive preprocessing‚Äîcleaning, sorting, and organizing‚Äîis required. This manual process is not only time-consuming but also prone to errors, making it a major bottleneck in research workflows. To address this issue, we have developed a Python-based algorithm that automates the cleaning and sorting of ASOS data, significantly improving efficiency and accuracy. The algorithm reads raw ASOS files and processes them based on user-defined criteria, allowing flexible and efficient data organization. Specifically, it sorts data according to meteorological variables (e.g., temperature, mean sea-level pressure, visibility, gust, wind speed, wind direction), time, and station location. Additionally, it detects and removes duplicate entries, ensuring data integrity and consistency. The structured output is then saved in separate folders based on the chosen sorting criteria, providing an organized dataset ready for analysis. One of the key strengths of this tool is its adaptability to various research needs. Users can customize the sorting and filtering process to focus on specific parameters relevant to their study. Whether analyzing long-term climatological trends, investigating extreme weather events, or validating numerical weather prediction (NWP) models, researchers can quickly extract and organize the data they need without the burden of manual preprocessing. Furthermore, by automating these tedious tasks, the algorithm reduces human errors and enhances the reproducibility of research findings. The algorithm was designed with usability in mind, making it accessible to students and researchers with varying levels of programming experience. It requires minimal setup and provides clear output structures, making ASOS data more accessible to a broader scientific community. Additionally, its efficiency in handling large datasets ensures that researchers can process weeks or even months of ASOS observations in a fraction of the time required for manual data handling. This research will discuss the implementation details of the algorithm, including its structure, optimization strategies, and performance when processing large ASOS datasets. As scientific software development continues to evolve, automation will play a crucial role in improving data-driven decision-making. This contribution aims to foster discussions on best practices in scientific software development and encourage collaborations to further enhance meteorological data processing tools. </p></details></div> </div> <div class=row> <div class=cell><b>3:40 PM</b></div> <div class=cell>Samuel Akinjole</div> <div class=cell-content><p></p> <details class="abstract cell"> <summary>Development of GEOS-Chem-hyd: Enabling Calculation of Numerically Exact Second-Order Sensitivities</summary> <p><strong>[<a href="https://drive.google.com/file/d/1Tu3Q8bcrKMmGapRJ3YSxsu96khipSK96/view?usp=drive_link">Slides</a>] - [<a href="https://www.youtube.com/watch?v=FwJNPHx86GY&t=26595s">Recording</a>]</strong></p> <p><em>Samuel Akinjole is a Ph.D. candidate in Environmental Engineering with a minor in Applied Data Science at Drexel University. He holds a bachelor's degree in Chemical Engineering, graduating with a First-Class honors from the University of Lagos, Nigeria. With an exemplary academic track record and expertise in high-performance computing, he has spearheaded the development of GEOS-Chem-hyd, enabling the calculation of second-order sensitivities in a global atmospheric model. Samuel is skilled in Python, Fortran, and Machine learning, with experience optimizing large-scale geospatial simulations. He has presented his work at prominent conferences, including the 11<sup>th</sup> International GEOS-Chem Meeting in St. Louis and the International HPC Summer School held in Kobe, Japan. His innovative projects span topics such as atmospheric modeling, data assimilation and machine learning.</em></p> <p>The development of GEOS-Chem-hyd introduces a transformative approach to sensitivity analysis in global atmospheric chemistry modeling through the use of hyperdual numbers. Hyperdual methods enable the precise computation of derivatives, including the Jacobian and Hessian, which are essential for higher-order sensitivity analysis. This project implements a novel tangent linear method for evaluating both first- and second-order sensitivities with machine precision in GEOS-Chem, building on prior successes in CMAQ-hyd (Liu et al., 2024). </p> <p>This presentation will discuss the technical and scientific innovations underlying GEOS-Chem-hyd, the challenges encountered during development, and the innovative solutions employed. Attendees will gain insights into integrating advanced mathematical tools, such as hyperdual methods, into legacy models like GEOS-Chem. We will also discuss strategies for maintaining accessibility and relevance in a rapidly evolving computational landscape. By improving sensitivity analyses through precise second-order derivatives, this work provides new insights into non-linear atmospheric processes and enhances the broader field of geoscientific modeling. </p></details></div> </div> <div class=row> <div class=cell><b>4:00 PM</b></div> <div class=cell></div> <div class=cell-content><p></p> <details class="success cell"> <summary>Notebook Proceedings Office Hours</summary> <p>During the Notebook Proceedings Office Hours, we will provide guidance on preparing and formatting Jupyter Notebook-based conference proceedings. Attendees can get help with markdown formatting, reproducibility best practices, code execution issues, and submission requirements to ensure their notebooks meet the conference standards. </p></details></div> </div> <div class=row> <div class=cell><b>5:00 PM</b></div> <div class=cell></div> <div class=cell-content><p></p> <details class="warning cell"> <summary>End of Day 2</summary> <p></p></details></div> </div><p></p> </div> </div> <div class=tabbed-block> <p></p><div class=table> <div class=row> <div class=cell><b>8:30 AM</b></div> <div class=cell>Damian Rouson</div> <div class=cell-content><p></p> <details class="abstract cell"> <summary>Cloud microphysics training and aerosol inference with the Fiats deep learning library</summary> <p><strong>[<a href="https://drive.google.com/file/d/17hRfifaNN-Lz9ZptOC9aoxbTS769CmXE/view?usp=drive_link">Slides</a>] - [<a href="https://www.youtube.com/watch?v=ZV1tyzzr0Ho&t=1963s">Recording</a>]</strong></p> <p><em>Damian Rouson is a Senior Scientist and the Group Lead for the Computer Languages and Systems Software (CLaSS) Group at Berkeley Lab. He is a mechanical engineer with experience in simulating turbulent flows in multiphase, quantum, and magnetohydrodynamic media. At Berkeley Lab, he researches language-based parallel programming and deep learning, teaches tutorials in parallel Fortran and UPC++, and leads open-source software projects including the Fiats deep learning library, the Caffeine parallel runtime library, and the Julienne unit-testing framework.</em></p> <p><em>He co-authored the textbook Scientific Software Design: The Object-Oriented Way (Cambridge University Press, 2011) and has taught courses and tutorials on object-oriented design patterns, parallel programming, and agile software development. He is an alternate member of the Fortran standard committee. He has held staff and faculty positions at the City University of New York, the University of Maryland, the University of Cyprus, the University of Bergen, and Stanford University. He has held staff and management positions at the U.S. Naval Research Laboratory and Sandia National Laboratories. He received a 2020-'21 Better Scientific Software Fellowship from the Exascale Computing Project and a 2025 Developer of the Year Award from Berkeley Lab‚Äôs Intellectual Property Office. He has been a (co-)principal investigator on research grants and research software engineering contracts funded by the Department of Energy, the National Institute of Standards and Technology, the National Science Foundation, the Office of Naval Research, the Nuclear Regulatory Commission, and the National Aeronautics and Space Administration.</em></p> <p>This talk will present two atmospheric sciences demonstration applications in the Fiats software repository (<a href=https://go.lbl.gov/fiats>https://go.lbl.gov/fiats</a>). Fiats, an acronym that expands to ‚ÄúFunctional inference and training for surrogates‚Äù or ‚ÄúFortran inference and training for science,‚Äù is a deep learning utility that targets high-performance computing applications in Fortran 2023. The first application trains a cloud microphysics neural-network surrogate model that has been integrated into the Berkeley Lab fork of the Intermediate Complexity Atmospheric Research (ICAR) model (<a href=https://go.lbl.gov/icar>https://go.lbl.gov/icar</a>). The second application performs parallel inference with an aerosol dynamics surrogate pretrained using data from the Energy Exascale Earth System Model (E3SM ‚Äì <a href=https://e3sm.org/ >https://e3sm.org/</a>). </p> <p>Fiats provides novel support for functional programming styles by providing inference and training procedures declared to be ‚Äúpure,‚Äù a language requirement for invoking a procedure inside Fortran‚Äôs loop-parallel construct: ‚Äúdo concurrent.‚Äù Because pure procedures clarify data dependencies, at least four compilers are currently capable of automatically parallelizing ‚Äúdo concurrent‚Äù on central processing units (CPUs) or graphics processing units (GPUs). The talk will present strong scaling results on a single node of Berkeley Lab‚Äôs Perlmutter supercomputer, showing near-ideal scaling up to 16 cores with additional speedup up to the hardware limit of 128 cores based on results obtained by compiling with a fork of the LLVM Flang Fortran compiler.</p> <p>Fiats provides a derived type that encapsulates neural-network parameters and provides generic bindings for invoking inference functions and training subroutines of various precisions. A novel feature of the Fiats design is that all procedures involved in inference and training are non-overridable, which eliminates the need for dynamic dispatch at call sites. In addition to simplifying the structure of the resulting executable program and potentially improving performance, we expect this feature to enable the automatic offload of inference and training to GPUs. </p> <p>The talk will conclude by presenting the use of ‚Äúdo concurrent‚Äù in a parallel training algorithm, highlighting the considerable simplifications afforded by the evolution of ‚Äúdo concurrent‚Äù from its introduction in Fortran 2008 to its enhancement in Fortran 2018 and further enhancement in Fortran 2023. </p></details></div> </div> <div class=row> <div class=cell><b>8:50 AM</b></div> <div class=cell>Daniel Abdi</div> <div class=cell-content><p></p> <details class="abstract cell"> <summary>Developing a Data-Driven Emulator for the High-Resolution Rapid Refresh (HRRR) Model</summary> <p><strong>[<a href="https://drive.google.com/file/d/1t_X9_XdHPYQr0ezcnA_3MMjO13C2TEsa/view?usp=drive_link">Slides</a>] - [<a href="https://www.youtube.com/watch?v=ZV1tyzzr0Ho&t=3331s">Recording</a>]</strong></p> <p><em>Daniel is a research scientist at CIRA currently working on developing data-driven models for medium-range weather forecasting for both global and regional applications. My previous background include accelerating several traditional NWP models on GPUs, contributing to R2O products of NOAA including GFS and RRFS, and developing AI agents for several games including chess and Go.</em></p> <p>The High-Resolution Rapid Refresh (HRRR) model is pivotal in operational weather forecasting, providing detailed and timely predictions across the contiguous United States (CONUS). To complement and enhance this process, our project focuses on developing a data-driven emulator for the HRRR model. Using state-of-the-art machine learning techniques, we aim to deliver computationally efficient alternatives that maintain or exceed the accuracy of traditional numerical models. This work leverages a large HRRR dataset, preprocessed into Zarr format with chunking optimized for efficient training and inference. </p> <p>Our approach explores two primary machine learning architectures: ResNet-based models (ResHRRR) and graph-based models (GraphHRRR). ResHRRR leverages convolutional neural networks and incorporates enhancements such as squeeze-and-excitation blocks and Feature-wise Linear Modulation (FiLM) to improve accuracy. GraphHRRR builds on previous work in global weather modeling, adapting it to rectangular CONUS domains with Delaunay triangulation and modified boundary handling. We first tested these models on subdomains (e.g., western U.S. and central U.S.) and achieved promising results in predicting key variables like wind speed and temperature. Notably, the GraphHRRR model outperformed ResHRRR in capturing high-resolution spatial details, particularly for composite reflectivity. Forecasts on the whole CONUS domain but at a reduced resolution of 6km instead of the native 3km resolution exhibited similar performance characteristics as the small subdomains.</p> <p>Initial results demonstrate the potential of these emulators in achieving fast and reliable forecasts with lead times up to 9 hours. Enhanced sharpness in predictions was achieved by integrating denoising diffusion models, which significantly improve the quality of forecasts for variables like reflectivity. Future work will optimize memory usage, and explore novel probabilistic methods such as GenCast and CorrDiff. By combining advanced data-driven approaches with traditional numerical weather prediction, this project aims to pave the way for more efficient and scalable operational forecasting systems. </p></details></div> </div> <div class=row> <div class=cell><b>9:10 AM</b></div> <div class=cell>David Hahn</div> <div class=cell-content><p></p> <details class="abstract cell"> <summary>¬µWMS: Cloud-Based Microservice Web Map Service</summary> <p><strong>[<a href="https://drive.google.com/file/d/14fWHTCeBl-X3aNqrwm68YCFyj_DE5g8K/view?usp=drive_link">Slides</a>] - [<a href="https://www.youtube.com/watch?v=ZV1tyzzr0Ho&t=4394s">Recording</a>]</strong></p> <p><em>David Hahn is a software engineer at UCAR, primarily in NCAR/RAL, but also working for UCP/COSMIC and UCP/JCSDA over the years. David has a passion for working in DevOps roles and cloud native deployments that take advantage of scalability, cost savings, and a reduction in system maintenance.</em></p> <p>The Web Map Service (WMS) plays a crucial role in integrating geographical datasets into cohesive visual displays within systems developed at NCAR and UCP. However, managing these systems presents a variety of challenges throughout the project lifecycle, including capacity planning, web server configuration, encryption certificate management, OS maintenance, and hardware acquisition and upkeep.</p> <p>In recent years, our work has highlighted the advantages of leveraging cloud hosting through Platform as a Service (PaaS) solutions. This approach simplifies capacity planning, eliminates the need for OS and web server configuration, and allows for a flexible pay-as-you-go model for computing and storage.</p> <p>Despite its importance, migrating the WMS to a serverless environment has proven challenging. To address this, RAL is spearheading a small initiative aimed at demonstrating the feasibility of deploying WMS using a microservice architecture in the cloud. This project utilizes AWS services, including API Gateway, Lambda, S3, and DynamoDB to create a scalable solution.</p> <p>The primary objective of this talk is to present a proof of concept, including results from load testing to evaluate scalability. Attendees will gain insights into our findings and updates from this innovative project, showcasing the potential of cloud-based WMS implementations. </p></details></div> </div> <div class=row> <div class=cell><b>9:30 AM</b></div> <div class=cell>Joshna Kurra</div> <div class=cell-content><p></p> <details class="abstract cell"> <summary>Toward Generating Experiment-Specific Notebooks in FABRIC</summary> <p><strong>[<a href="https://drive.google.com/file/d/15KWO23ZrA3ujan5EKrcmpahqJaJLpYdZ/view?usp=drive_link">Slides</a>] - [<a href="https://www.youtube.com/watch?v=ZV1tyzzr0Ho&t=5622s">Recording</a>]</strong></p> <p><strong>Coauthors: Mami Hayashida, Joshna Kurra, Zongming Fei, James Griffioen</strong></p> <p>Joshna is a recent graduate with a Bachelor‚Äôs degree from the University of Kentucky. She is currently in her first semester of graduate school, studying Data Science. She began working with FABRIC in January 2024. Initially, she assisted in creating and updating teaching materials‚Äîexperiments designed for professors to use in their classes to teach concepts in networking‚Äîmaintained by FABRIC. Currently, she is working on the Jupyter Notebook Generation project presented in the abstract. Her research interests include Generative AI, data mining and data analysis.*</p> <p>Jupyter notebooks are now widely used by the research community to set up, launch, run, analyze, and document scientific experiments. Jupyter notebooks also allow scientific software and experiments to be easily shared by researchers and has resulted in extensive shared notebook repositories. The massive number of example notebooks available not only has made it easier for researchers to write notebooks, but also represents a wealth of data that can be used by Generative AI systems to automatically generate experiment-specific notebooks. This paper describes the use of RAG-based AI techniques to automatically generate jupyter notebooks in the context of the NSF FABRIC testbed.</p> <p>FABRIC is a next generation network testbed that consists of over 30 sites across the U.S., Asia, and Europe, including many supercomputing facilities and other specialized testbeds. Each FABRIC node (‚Äúrouter‚Äù) is an advanced compute cluster with GPUs, FPGAs, programmable NICs, and large amounts of storage. In this sense, FABRIC can be regarded as a federation of HPC-style resources that can be programmed ‚Äì including the network. </p> <p>In order to program the FABRIC testbed, including reserving and managing resources, researchers must use FABlib, a python API. While there is an extensive collection of example JupyterHub notebooks and documentation demonstrating this usage, the learning curve for first-time users can be steep, a challenge often seen in other HPC environments. Moreover, as the capabilities and scope of FABRIC expands, finding the necessary information from notebook usage examples and combining different elements becomes increasingly more complex and difficult. To address this problem, we have implemented an AI-based tool, leveraging the power of LLMs and Retrieval Augmented Generation(RAG), that generates a draft Python notebook based on the user's request. Automating the routine steps that researchers would have to take when getting started with FABRIC, we can give them a chance to be more focused on their experiments. </p> <p>In our talk, we will present the architecture of the application, design decisions we have made, and the user-side experience. We will also discuss such details as the different LLM models tested, RAG techniques explored, prompt engineering, and finally, the challenges we have faced as we are continuing to improve its performance. Since RAG is a technique that is recently being adapted in various fields to try and create specialized chatbots, we hope this talk will be a helpful guide to the audience if they decide to take upon such a project in their own research area. </p></details></div> </div> <div class=row> <div class=cell><b>9:50 AM</b></div> <div class=cell></div> <div class=cell-content><p></p> <details class="info cell"> <summary>Break</summary> <p></p></details></div> </div> <div class=row> <div class=cell><b>10:20 AM</b></div> <div class=cell>Brad Klotz</div> <div class=cell-content><p></p> <details class="abstract cell"> <summary>An Interconnected Workflow Design for Simulating Airborne Phased Array Radar (APAR) Data</summary> <p><strong>[<a href="https://drive.google.com/file/d/1re7YtS0C7ycmXW84qNwDY9zQZlDtaept/view?usp=drive_link">Slides</a>] - [<a href="https://www.youtube.com/watch?v=ZV1tyzzr0Ho&t=8527s">Recording</a>]</strong></p> <p><em>Brad Klotz is a Project Scientist within the Remote Sensing Facility of NSF NCAR's Earth Observing Laboratory. During his four and a half years at NSF NCAR, Brad has worked primarily on the Airborne Phased Array Radar (APAR) program, supporting APAR simulation software development, data generation, algorithm development, and overall science team logistics and coordination. Throughout his career, Brad has devoted much of his time and energy on supporting airborne remote sensing, especially pertaining to severe weather and tropical weather systems.</em></p> <p>The Airborne Phased Array Radar (APAR) program is currently under development through support from NSF and NOAA. This transformational radar will become the first airborne, C-band, phased array weather radar for use within the scientific community. While the hardware and associated aircraft modifications are still in development, it is critical for scientists and engineers to have an understanding of the performance capabilities of the radar as it pertains to scanning, data collection and processing, and scientific applications. There are many available scientific applications for radar data, many of them held within the NSF NCAR supported Lidar Radar Open Software Environment (LROSE). Data quality control, 3-D wind analyses, and discrimination of particle types in LROSE are several examples of tools that are useful for APAR. These applications serve as an end goal for the utilization of the collected data.</p> <p>To provide research quality data in the current stage of APAR development, an APAR scientific simulator was developed. Known as the APAR Observing Simulation, Processing, and Research Environment (AOSPRE, pronounced ‚ÄúA-Osprey‚Äù), this tool is able to read in high-resolution numerical weather model data, such as from the Weather Research and Forecasting (WRF) model, and compute radar moments in the context of the APAR scanning parameters and characteristics. This software environment is designed to serve as an interconnecting framework of existing software tools and modules in order to provide a seamless workflow of simulated radar data generation. Currently, AOSPRE is broken into three main software blocks, which includes a pre-processing phase, a radar simulation phase, and a data processing and applications phase. For this presentation, a detailed discussion of the design of each phase in the workflow will be provided. For the pre-processing phase, a user points to a database of high-resolution numerical model output and can design a relevant flight plan in and around the weather of interest. This flight planning tool has recently advanced to a GUI that allows either manual or machine-learning based guidance on the most ideal path. The GUI application was developed in Matlab but can be operated within a container. Information on the flightpath and desired scanning patterns are stored for use in a namelist file that is accessed in phase two of the workflow. The main portion of the AOSPRE codebase controls reading in the weather model output, operating the designated flight, generating the APAR output, and saving the output in CfRadial files. This code is currently written in Fortran and incorporates multiple subroutines and modules that handle different aspects of determining the aircraft location, scanning, and conversion from model to radar coordinates. It also uses an existing moment generator, called the Cloud-resolving Radar SIMulator (CR-SIM), for final computation of the radar moments. This portion of AOSPRE also utilizes parallel computing resources based in OpenMP to improve processing efficiency. The output is then linked to other tools and applications to understand the uncertainty in the measurements or new ways to apply the data. A brief description of some future design updates of the code will also be provided. As a whole, the AOSPRE code follows LROSE as an example for open source software development but with the specific intent to provide support to the APAR program and future interaction with field campaign planning and operations. </p></details></div> </div> <div class=row> <div class=cell><b>10:40 AM</b></div> <div class=cell>Dazhong Xia</div> <div class=cell-content><p></p> <details class="abstract cell"> <summary>A small tool for medium data: browser-based data exploration and export done quick</summary> <p><strong>[<a href="https://drive.google.com/file/d/1q97loYyuiP8v6jnm9-d90gk5jXxZE7C9/view?usp=drive_link">Slides</a>] - [<a href="https://www.youtube.com/watch?v=ZV1tyzzr0Ho&t=9771s">Recording</a>]</strong></p> <p><em>Dazhong is a member of Catalyst Cooperative, a worker-owned cooperative that wrangles data about the US energy system. He has over a decade of software experience that spans data engineering, devops, and full-stack web development.</em></p> <p>Catalyst Cooperative publishes the Public Utility Data Liberation project (PUDL), a collection of data about the US energy system. It is over 200 tables, with thousands of different columns, a scale that can be overwhelming to navigate. We asked our users what they needed for the data to be accessible and usable, and found some common use cases. They wanted to search through the available data, filter and explore different datasets, and then export the data they found as CSV.</p> <p>Existing data exploration and metadata search tools didn‚Äôt meet those needs. They‚Äôre typically designed for internal corporate use, and run into many challenges in an open data context while being over-engineered for our relatively modest data sizes. Additionally, exploration tools had poor support for metadata search, metadata tools had poor exploration support, and no tools had good support for exporting large amounts of data to CSV.</p> <p>We built an open-source tool that provides the search, exploration, and export functionality that our users need. It also requires minimal resources, is contained in several hundred lines of code, and can track usage metrics on a per-user basis. In this talk we‚Äôll go over how the tool works and how you can adapt it to the data you publish. </p></details></div> </div> <div class=row> <div class=cell><b>11:00 AM</b></div> <div class=cell>Zach Schira</div> <div class=cell-content><p></p> <details class="abstract cell"> <summary>Building, Maintaining, and Tracking Machine Learning Models in a Production Environment</summary> <p><strong>[<a href="https://drive.google.com/file/d/1N4Z3dEhOn8geWdbEqFVS1GmbT41vbMjD/view?usp=drive_link">Slides</a>] - [<a href="https://www.youtube.com/watch?v=ZV1tyzzr0Ho&t=10921s">Recording</a>]</strong></p> <p><em>Zach Schira is a data engineer at Catalyst Cooperative where he has helped to build out infrastructure enabling the team to tackle large, complex machine learning problems. Recently, he lead the infrastructure development for a project that involved training and deploying a model to extract data from over 300,000 PDF's.</em></p> <p>With the proliferation of ‚Äúoff the shelf‚Äù machine learning models, developers can quickly and easily get models prepared and finetuned for a variety of tasks. This means even relatively small teams can end up managing many different models applied to a diverse set of problems. While this new paradigm allows teams to solve interesting and novel problems, a lack of organization can cause these models to become unruly and disorganized without proper forethought into their design and management. For example, if each model is developed with a bespoke system for saving weights, testing performance, and deployment in a production environment, the maintenance burden for managing such a setup can balloon exponentially. Fortunately, there‚Äôs a growing suite of open source tools to help teams manage their models in a consistent, effective way.</p> <p>This talk will focus on using the library ‚Äòmlflow‚Äô to manage machine learning models. The talk will present a notebook that demonstrates the various uses of mlflow by working through a simple classification problem. This will include training, testing, and comparing models pulled from multiple different machine learning frameworks. After working through this demonstration, the talk will then focus on integrating models managed by mlflow into a production environment. This discussion will also touch on other common tools in an open-data stack, and how mlflow can be used alongside these tools. By end of this talk, the audience should feel comfortable developing machine learning models that can be maintained and used well into the future. </p></details></div> </div> <div class=row> <div class=cell><b>11:20 AM</b></div> <div class=cell>Edward Hartnett</div> <div class=cell-content><p></p> <details class="abstract cell"> <summary>Using AI in Earth Science Programming</summary> <p><strong>[<a href="https://drive.google.com/file/d/1y5AqTQDFFiokRvO07hRWEotd8sVOIc4U/view?usp=drive_link">Slides</a>] - [<a href="https://www.youtube.com/watch?v=ZV1tyzzr0Ho&t=11981s">Recording</a>]</strong></p> <p>*Edward Hartnett is an author of NetCDF, a freely available software library for scientific data, used by NASA, NOAA, the ESA, and climate and meteorology scientists around the world. He contributed to the netCDF-4 upgrade, as well as many other features, tests, and documentation.</p> <p>While working at LASP, he wrote software for the ground data processing systems of several NASA missions, including data processing for the Total Irradiance Monitor (TIM) and Spectral Irradiance Monitor (SIM); these instruments were used in several missions, including SORCE, TCTE, and TSIS-1 and 2. He also works on improving parallel IO performance for the scientific modeling community as a co-author of the Parallel IO library (PIO), a C/Fortran library that runs on supercomputers, as part of weather and climate models.</p> <p>Recently, he has begun working on NOAA‚Äôs NCEPLIBS, a collection of libraries that are used in the Unified Forecast Model (UFS), as well as many other weather and climate models and applications. He has also supervised or led several excellent software engineering teams, including the Production Software Team, as part of Ground Data Systems for several NASA missions, including Messenger, Cassini, MAVEN, MMS, SORCE, AIM, and TSIS, and the NCEPLIBS team at NOAA.*</p> <p>NOAA‚Äôs Environmental Modeling Center (EMC) maintains many Fortran and C codes developed which are part of the Unified Forecast System (UFS). These codes are of vital importance for the daily forecast and many other NOAA products. To continue to maintain and extend these codes with the most productive and useful tools, we have been examining the use of AI tools in the maintenance of Earth science software. AI tools have the capability to transform the work of software engineers and managers. Available tools include:</p> <ul> <li>predictive code completion</li> <li>generation of text, audio, video, and slides</li> <li>code inspection and improvement</li> <li>debugging</li> <li>test generation</li> </ul> <p>In full accordance with NOAA‚Äôs policies on the use of generative AI, we have been experimenting with these tools in our software development efforts. In this presentation we demonstrate some of these capabilities and relate whether and how they contribute to our productivity and the quality of our code. </p></details></div> </div> <div class=row> <div class=cell><b>11:40 AM</b></div> <div class=cell>Daralynn Rhode</div> <div class=cell-content><p></p> <details class="abstract cell"> <summary>IMAP Data Access API: Functionality, Improvements, and Usages For Better Data Management</summary> <p><strong>[<a href="https://drive.google.com/file/d/1zpOAVAVuJflmf5C2R8KgEMk8gdASlumB/view?usp=drive_link">Slides</a>] - [<a href="https://www.youtube.com/watch?v=ZV1tyzzr0Ho&t=13463s">Recording</a>]</strong></p> <p><em>Daralynn is a junior studying Computer Science and Astronomy at CU Boulder. She currently works as a student developer at the Laboratory for Atmospheric and Space Physics (LASP), contributing to the Science Data Center for the Interstellar Mapping and Acceleration Probe (IMAP) mission.</em></p> <p>As space research missions generate increasingly complex datasets, the IMAP Data Access API aims to simplify how scientists and researchers interact with the data.</p> <p>The Interstellar Mapping and Acceleration Probe Mission, launching in 2025, utilizes an API to facilitate access of data stored in the cloud. The first part of the talk will cover expanded API features for more functional and user friendly queries, and discuss [upcoming] support for ancillary file uploads, downloads, querying, and storage, allowing for better management of supplementary files alongside traditional data records. The second part will highlight how these features strengthen and simplify the data management of the L1 through L3 data products and streamline data discovery and retrieval for users. By enabling cloud-based access to the full scope of mission data, the API facilitates interaction with complex datasets, ensuring researchers can quickly upload, query, and download the data they need. Lastly the talk will preview upcoming enhancements to further strengthen IMAP‚Äôs data management system. As an undergraduate student project, this work contributes to the larger efforts of the IMAP science team, supporting their goal of improving data and open source code accessibility and usability for scientific research. </p></details></div> </div> <div class=row> <div class=cell><b>12:00 PM</b></div> <div class=cell></div> <div class=cell-content><p></p> <details class="info cell"> <summary>Lunch</summary> <p></p></details></div> </div> <div class=row> <div class=cell><b>1:00 PM</b></div> <div class=cell>Jon Rood</div> <div class=cell-content><p></p> <details class="note cell"> <summary>Accelerating Software Development with Spack</summary> <p><em>Jon Rood is a computational scientist at NREL. He currently works on performance engineering of next-generation applications for wind energy applications. He has also previously worked on high-performance computing projects for agent-based simulations of ancient societies at Argonne National Laboratory, instrument analysis applications for NASA while at Tech-X Corporation, bioinformatics applications while at Lawrence Berkeley National Laboratory, and weather and climate applications while at ETH Z√ºrich.</em></p> <p>Modern scientific software development in high performance computing requires integration of multiple software projects, use of multi-device programming models, and portability requirements across multiple machines. The Spack project has features which make software development in this complex environment simpler and more efficient. However, Spack comes with its own set of complexities and mechanisms which can deter new users from truly embracing the project. Fortunately some of these complexities can be simplified through extensions to Spack, as well as through the experience of using Spack in one's own project.</p> <p>In this short tutorial we will focus on demonstrating how to effectively use the software development features of Spack. We will also demonstrate how to add capability on top of Spack using extensions to automate configuration of machines used within a software project. We will show how this same development framework can be used for automating software testing and deployment. Participants will unravel how to turn Spack into the most convenient and extensible software development framework for high performance computing applications that exists today. </p></details></div> </div> <div class=row> <div class=cell><b>3:00 PM</b></div> <div class=cell></div> <div class=cell-content><p></p> <details class="info cell"> <summary>Networking Event</summary> <p></p></details></div> </div> <div class=row> <div class=cell><b>5:00 PM</b></div> <div class=cell></div> <div class=cell-content><p></p> <details class="warning cell"> <summary>End of Day 3</summary> <p></p></details></div> </div><p></p> </div> </div> <div class=tabbed-block> <p></p><div class=table> <div class=row> <div class=cell><b>8:30 AM</b></div> <div class=cell></div> <div class=cell-content><p></p> <details class="info cell"> <summary>Closing Remarks</summary> <p></p></details></div> </div> <div class=row> <div class=cell><b>8:40 AM</b></div> <div class=cell>Ligia Bernardet</div> <div class=cell-content><p></p> <details class="abstract cell"> <summary>Invited Keynote: Earth System Model Development Activities at the NOAA Global Systems Laboratory - A software engineering perspective</summary> <p><strong>[<a href="https://drive.google.com/file/d/1Ts5m5w5TF3LsBb7hNFUTVaUWAaM5epsV/view?usp=drive_link">Slides</a>] - [<a href="https://www.youtube.com/watch?v=jdpMXYFufBs&t=2281s">Recording</a>]</strong></p> <p><em>Ligia Bernardet is a meteorologist with a PhD in atmospheric sciences from Colorado State University. She is the chief of the Earth Prediction Advancement Division within the NOAA Global Systems Laboratory in Boulder, CO, a member of the Unified Forecast System (UFS) steering committee, and co-leads the Community Earth System Model (CESM) Software Engineering Working Group. Her experience in code management and user support for community models has given her an appreciation for the importance of software engineering best practices.</em></p> <p>NOAA GSL develops Earth System prediction models on a variety of spatiotemporal scales (large-eddy to seasonal), readiness levels (basic research to near-operational), and approaches (physical and data-driven). Physical models use diverse infrastructures, such as the Unified Forecast System (UFS), the Weather Research and Forecast (WRF) model, and the Model for Prediction Across Scales (MPAS). All models are evaluated for computational and scientific performance and must produce actionable information for users and stakeholders. Excellence in development relies on best practices in software engineering, such as component modularity with clear interfaces and sound code management with robust testing. The need for best practices is heightened by the distributed nature of our development, which includes partners from other NOAA entities, NCAR, and the broader community. This talk will describe the model development activities at NOAA GSL and provide highlights of selected efforts, including the management of physical parameterizations using the Common Community Physics Package (CCPP), the orchestration of scientific workflows with the Unified Workflow Tools package, and the training of machine learning models for weather prediction. </p></details></div> </div> <div class=row> <div class=cell><b>9:50 AM</b></div> <div class=cell></div> <div class=cell-content><p></p> <details class="info cell"> <summary>Break</summary> <p></p></details></div> </div> <div class=row> <div class=cell><b>10:20 AM</b></div> <div class=cell>Dom Heinzeller</div> <div class=cell-content><p></p> <details class="abstract cell"> <summary>The role of spack-stack in the operational implementation of the NAVY's next-generation weather forecasting model</summary> <p><strong>[<a href="https://drive.google.com/file/d/1IT6SzLXl5ppcwZhGRtZ5fMD1_LLUFX5A/view?usp=drive_link">Slides</a>] - [<a href="https://www.youtube.com/watch?v=jdpMXYFufBs&t=8415s">Recording</a>]</strong></p> <p><em>Dom Heinzeller graduated from Heidelberg University, Germany, with a PhD in Theoretical Astrophysics. Following a postdoctoral fellowship on the physical and chemical evolution of protoplanetary disks at Kyoto University, Japan, he moved into the field of Numerical Weather prediction in a number of organizations: The Forecasting Research Group of the National Weather Service of New Zealand in Wellington, New Zealand; the Institute for Meteorology and Climate Research of the Karlsruhe Institute of Technology in Garmisch-Partenkirchen, Germany; NOAA's Global Systems Laboratory in Boulder, CO; the Joint Center for Satellite Data Assimilation in Boulder, CO; the United States Naval Research Lab through a contract with UCAR CPAESS in Boulder, CO. He is the original author of the Common Community Physics Package (CCPP) used in the UFS, in NEPTUNE, in DTC's Single Column Model, and soon in CESM, and he is one of the founders of the the spack-stack collaboration.</em></p> <p>In 2025, the United States Naval Research Lab (NRL) is transitioning its next-generation numerical weather prediction (NWP) system NEPTUNE (Navy Environmental Prediction sysTem Using a Nonhydrostatic Engine) to their operational partner, Fleet Numerical Meteorology and Oceanography Center (FNMOC). The transitioning of innovations from research to operations (R2O) and the feedback from operations to research (O2R) is one of the major challenges in the NWP world, to the extent that it is sometimes referred to as the "valley of death". One of the contributors to this challenge is that the environment in which the NWP systems operate can differ greatly. The development and testing often take place on different high-performance computing (HPC) systems, with varying environments of software (also referred to as software stacks), and with different workflows than in operations.</p> <p>For the first operational implementation of NEPTUNE in 2025, NRL and FNMOC are adopting a novel approach to address some of these challenges: First, FNMOC will be running on highly secured, fenced-off partitions on the same HPC systems that NRL uses for its development. Second, NRL will provide a software environment based on spack-stack. The spack-stack project is a joint effort of several major federal agencies and UCAR labs. It leverages the spack package manager developed by Lawrence Livermore National Laboratory and supported by a large community. Spack-stack utilizes modern software development practices like continuous integration (CI) and a fast release cycle. Through the use of spack source mirrors and binary caches, FNMOC will be able to deploy bit-for-bit identical software environments in their secured environments in a matter of minutes.</p> <p>In this presentation, we will revisit the challenges in the R2O2R cycle in NWP, provide an overview of the joint spack-stack effort, and cover the deployment of NEPTUNE in operations.</p> <p>[Distribution Statement Approved for public release. Distribution is unlimited.] </p></details></div> </div> <div class=row> <div class=cell><b>10:40 AM</b></div> <div class=cell>Carol Ruchti</div> <div class=cell-content><p></p> <details class="abstract cell"> <summary>Modernizing Product Generation for the NSF NCAR EOL Field Catalog</summary> <p><strong>[<a href="https://drive.google.com/file/d/1l_BZFyq8dXP0mtJFPTJQOvm1MPYS2VRe/view?usp=drive_link">Slides</a>] - [<a href="https://www.youtube.com/watch?v=jdpMXYFufBs&t=9776s">Recording</a>]</strong></p> <p><em>Carol Ruchti is an associate scientist at NCAR/EOL. She has worked with the EOL for the past 7 years as a manager for EOL Field Catalog. During her time at NCAR, she has supported in field campaigns across the globe, lead the NCAR Early Career Scientist Assembly (ECSA), participated in the EOL's Council for Representation Engagement, and Well-being (CREW), and continues to learn more about software engineering.</em></p> <p>The NSF NCAR EOL Field Catalog is a customizable tool to allow science teams to document field project operations by collecting and displaying products. These products can be satellite imagery, radar plots, weather balloon SkewTs, field reports, etc. In most cases, these products for a given field project‚Äôs field catalog are all generated by the EOL catalog admins staff by either pulling products from the web or producing plots from scientific data.</p> <p>The field catalog was developed back in 1995, and there have been two official UI versions of the field catalog released in 1995 and 2013, but there haven‚Äôt been many major updates to the code that generates the products that are displayed in the field catalog. These products were mostly generated using perl code that was copied with minor changes unique to each field campaign. For the last 30 years, this perl code has served the EOL catalog team very well, but there was a desire to modernize this code using object oriented principles in Python. Thus, after working directly with an EOL Software Engineering mentor, a new GitHub repository of Python code was developed. Within the new Python code, an FCProduct class was defined, issues were tracked, branches were created, and the importance of exception handling was learned. The journey of creating this new object oriented python code for field catalog product generation highlights the importance of continuing to modernize our code at NSF NCAR and provides the capability to create more unique and complex products for the EOL Field Catalog into the future. </p></details></div> </div> <div class=row> <div class=cell><b>11:00 AM</b></div> <div class=cell>Daniel Madren</div> <div class=cell-content><p></p> <details class="abstract cell"> <summary>From Chaos to Clarity: Structuring Research Software Engineering Projects for Success</summary> <p><strong>[<a href="https://drive.google.com/file/d/1FhgH2dK8oTUA7Qd8vVQ_IAAOYAf08ebL/view?usp=drive_link">Slides</a>] - [<a href="https://www.youtube.com/watch?v=jdpMXYFufBs&t=10856s">Recording</a>]</strong></p> <p><em>Daniel joined the Purdue Research Computing team in April 2024 as the Program Manager for the newly established RSE Center. In this role, he oversees the development of project management processes, including project intake, rate setting, and business plan strategizing, to engage both internal stakeholders at Purdue and potential external clients. Daniel also serves as the co-chair for the USRSE Group Leaders Network, contributing to leadership and collaboration across the research software engineering community.</em></p> <p><em>At SC24, Daniel delivered a talk on self-sustaining operational models and growth strategies, showcasing innovative approaches to fostering sustainability and scalability in the field. Committed to more than just enhancing operational efficiency, Daniel strives to inspire and establish sustainable, innovative practices that set new benchmarks for excellence within the research software engineering community.</em></p> <p>Research Software Engineering (RSE) projects exist at the intersection of technical innovation and interdisciplinary collaboration, presenting unique management challenges. Without clear structures, teams often face misaligned goals, miscommunication, scope creep, inconsistent documentation, and difficulty scaling efforts. This presentation will explore how modern project management practices, tailored to the distinct needs of RSE, can transform chaos into clarity.</p> <p>We‚Äôll highlight the role of structured frameworks, including hybrid methodologies and well-crafted Statements of Work (SOWs), in addressing these challenges. SOWs provide clarity and direction, especially in the absence of traditional grant funding, by establishing expectations, timelines, and objectives. Real-world scenarios and case studies will demonstrate how these approaches reduce inefficiencies, standardize workflows, and ensure reproducibility.</p> <p>Key topics will include effective documentation practices, hybrid project management techniques, and strategies to reduce technical debt. Whether managing a small RSE team or leading large-scale software initiatives, attendees will gain actionable insights into creating scalable, sustainable, and collaborative workflows that align with the demands of modern scientific software development. </p></details></div> </div> <div class=row> <div class=cell><b>11:20 AM</b></div> <div class=cell>Phil Du</div> <div class=cell-content><p></p> <details class="abstract cell"> <summary>A Practical Framework for Small Teams to Develop Sustainable Research Software</summary> <p><strong>[<a href="https://drive.google.com/file/d/13pTt9Y9E9XytgaYSLvaW7F4itsi_L9mE/view?usp=drive_link">Slides</a>] - [<a href="https://www.youtube.com/watch?v=jdpMXYFufBs&t=12270s">Recording</a>]</strong></p> <p><em>Phil is a current graduate student in Computing and Software at McMaster University.</em></p> <p>Currently, a knowledge gap exists between research software and general software engineering. The scientific literature is full of ideas to close this gap, including documentation templates, code generation, continuous integration/deployment and formal methods. Although these ideas are promising, they often assume a large team that includes individuals who have the required expertise.</p> <p>Our proposed practical framework instead targets a small team of domain experts, with the only requirement being that someone be found (either from the original team, or externally added) who is interested in deepening their software knowledge by volunteering for the developer role. Our framework, especially for the beginning stage of requirements elicitation, includes step-by-step guidance. The process begins with questions the developer asks the domain expert(s). These questions cover topics such as the expected inputs and outputs, the computational scale of the problem and special input cases with known solutions or trends. The methodology shows how to map the answers to these questions to the requirements, high-level design and verification documentation. Templates for all documentation, in markdown format, are provided in a GitHub template, along with the initial infrastructure for issue tracking and continuous deployment of the project's webpage. The proposed methodology incorporates four main pieces of advice: i) the notation and structure for documenting the theory should be selected to facilitate the transition to design and implementation; ii) continuous integration should be part of the project from the start; iii) the low-level design documentation should be done through structured comments in the code, like docstrings or doxygen; and, iv) the modular decomposition needs to consider the computational scale when balancing information hiding and performance. </p> <p>Our talk will present the details of the framework, examples of its application and initial plans for experiments to assess its effectiveness. </p></details></div> </div> <div class=row> <div class=cell><b>11:40 AM</b></div> <div class=cell>Maria Patrou</div> <div class=cell-content><p></p> <details class="abstract cell"> <summary>Modernization and Standardization of Software tools in Spectroscopy</summary> <p><strong>[<a href="https://drive.google.com/file/d/1jnGY6dvzRj0LAqjLEU5bYOSP0ltTEVUp/view?usp=drive_link">Slides</a>] - [<a href="https://www.youtube.com/watch?v=jdpMXYFufBs&t=13565s">Recording</a>]</strong></p> <p>**Coauthors: Kyle Ma, Andrei Savici*</p> <p><em>Dr. Maria Patrou joined Oak Ridge National Laboratory (ORNL) in 2023. Her focus is on designing, developing, and deploying data reduction and data analysis software for the neutron science community at the ORNL facilities. Dr. Patrou graduated from the National and Kapodistrian University of Athens in Greece, with a Bachelor's in Informatics and Telecommunications in 2012. Additionally, she earned her Master's and Ph.D. in Computer Science at the University of New Brunswick in Canada. She defended her Ph.D. thesis with the title of "Efficiency and Performance Architecture Optimization of Node.js Applications under Parallel and Scalable Conditions" in April 2022.</em></p> <p><em>Dr. Patrou has worked as a Software Architect and Developer at Dot By Dot Business and Technology Solutions LTD, in Athens, Greece. During her studies at the University of New Brunswick, she worked as a Graduate Teaching Assistant and Instructor, while being part of IBM Centre for Advanced Studies - Atlantic (CASA), she collaborated with business partners, such as IBM Canada and The Black Arcs. After her Ph.D. completion, she continued to be part of CASA as a Research Assistant and participated on a project affiliated with the 45Drives company.</em></p> <p>Oak Ridge National Laboratory has two of the most advanced neutron scattering facilities in the world: the High Flux Isotope Reactor (HFIR) and the Spallation Neutron Source (SNS). Together they provide 30 instruments to enable studies of materials, using different experimental techniques, such as direct/indirect spectroscopy, triple axis spectrometers etc. Research scientists from around the world use various software tools to fetch, refine, and analyze neutron scattering data, when conducting their research. As software tools advance, and new updates are introduced, legacy tools need to be updated to follow modern software guidelines. In this talk, we will walk through the modernization process of legacy Spectroscopy tools, including deployment challenges and code quality improvements. Finally, we will present the way that the shared techniques and guidelines that we use to update the software can become a standard to follow naturally for future ones. </p></details></div> </div> <div class=row> <div class=cell><b>12:00 PM</b></div> <div class=cell></div> <div class=cell-content><p></p> <details class="info cell"> <summary>Lunch</summary> <p></p></details></div> </div> <div class=row> <div class=cell><b>1:20 PM</b></div> <div class=cell>Julia Sloan</div> <div class=cell-content><p></p> <details class="abstract cell"> <summary>Leveraging Modern Computing and Calibration-driven Development for Climate Modeling at CliMA</summary> <p><strong>[<a href="https://drive.google.com/file/d/1_C1rEe8HeEjgWOJ9iX4Ilu8ul4B4vKvd/view?usp=drive_link">Slides</a>] - [<a href="https://www.youtube.com/watch?v=jdpMXYFufBs&t=19192s">Recording</a>]</strong></p> <p><em>Julia is a research software engineer working in the Climate Modeling Alliance (CliMA) at Caltech. She leads the group‚Äôs coupling team in developing ClimaCoupler.jl, and also contributes software support for the land model, ClimaLand.jl. She‚Äôs passionate about applying high-performance computing to solve real-world problems, specifically in the interdisciplinary realm of climate sciences. Outside of work, Julia is an avid surfer, volunteer coach for Caltech‚Äôs water polo team, and ardent fan of her cat Comet.</em></p> <p>Today‚Äôs Earth System Models (ESMs) are primarily process-based models that allow scientists to understand the evolution of our planet‚Äôs climate on seasonal to centennial timescales. Many existing ESMs are based on legacy codes dating back decades. These codes often require hundreds to thousands of CPUs to run, predate modern software design principles, and are typically manually tuned with a fraction of the Earth observations currently available. The Climate Modeling Alliance (CliMA) is developing an open source ESM from scratch, with the intention of providing the wider community with access to a model that leverages modern computing capabilities, including cloud GPUs, and enables rapidly iterated learning from the wealth of Earth observations now available.</p> <p>This talk will provide an overview of the CliMA model and software ecosystem. It focuses on the project‚Äôs software infrastructure and integrated pipeline for automated calibration and uncertainty quantification of model components with data. As the ESM itself is computationally intensive to run, the calibration also becomes very expensive, even when leveraging GPU resources. To address this problem, we have developed methods to use ML-based emulators to accelerate model calibration and uncertainty quantification, which we are able to use within an iterative cycle during the model development process. </p></details></div> </div> <div class=row> <div class=cell><b>1:40 PM</b></div> <div class=cell>Erik Kluzek</div> <div class=cell-content><p></p> <details class="abstract cell"> <summary>Helping Scientists Embrace their Inner Research Software Engineer (RSE) and Working Together with the Community Earth System Model (CESM) RSE's to improve CESM Science; From a Land Component Model (CTSM) Perspective</summary> <p><strong>[<a href="https://drive.google.com/file/d/1tWercFlWkF6DgL7iktaEUVTgDLegzeii/view?usp=drive_link">Slides</a>] - [<a href="https://www.youtube.com/watch?v=jdpMXYFufBs&t=20391s">Recording</a>]</strong></p> <p><em>Erik Kluzek has been working as a Research Software Engineer for four decades. Working alongside scientists for that time and with an education coming from the science side. At one point he decided that Research Software was the place to put his focus. RSE Development and the Research Software he'd been working on needed attention and improvement. And it's a fun place to focus and a challenging problem, with lots of new learning to do along the way.</em></p> <p>The Community Earth System Model (CESM) is science expressed in Software, which means everyone who touches CESM code to work on the science -- is doing RSE work. There is a list of challenges that CESM RSE‚Äôs have with both bringing in new science for CESM and ensuring CESM is well tested, robust, flexible as well as having the correct science. As such CESM Scientists working in CESM code need to work alongside CESM RSE‚Äôs to adopt RSE practices while their science is being developed. For CESM to be useful for scientists it needs to be: well-tested, robust, reproducible, flexible and have assurance that the science is implemented correctly. This is needed both for the science and the software of CESM. Helping scientists embrace their inner RSE is an effort to keep CESM as a leading model in Earth System Science research, by having more people who work with the CESM code adopt good coding practices. In this talk I will specifically talk about RSE practices for the Land Model component of CESM, the Community Terrestrial Science Mode (CTSM)l. I will present on the RSE challenges we have in CTSM and how to solve them we need to involve and educate CTSM scientists to improve our RSE practices. I will also present regarding the CTSM RSE team‚Äôs efforts to educate the scientists and on some improvements to our RSE practices that we‚Äôve been implementing on our CTSM Software Development. </p></details></div> </div> <div class=row> <div class=cell><b>2:00 PM</b></div> <div class=cell></div> <div class=cell-content><p></p> <details class="success cell"> <summary>Notebook Proceedings Office Hours</summary> <p>During the Notebook Proceedings Office Hours, we will provide guidance on preparing and formatting Jupyter Notebook-based conference proceedings. Attendees can get help with markdown formatting, reproducibility best practices, code execution issues, and submission requirements to ensure their notebooks meet the conference standards. </p></details></div> </div> <div class=row> <div class=cell><b>3:00 PM</b></div> <div class=cell></div> <div class=cell-content><p></p> <details class="warning cell"> <summary>End of the Conference</summary> <p></p></details></div> </div><p></p> </div> </div> </div> </div> <aside class=md-source-file> <span class=md-source-file__fact> <span class=md-icon title="Last update"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"></path></svg> </span> <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-timeago" title="April 29, 2025 00:46:39 UTC"><span class=timeago datetime=2025-04-29T00:46:39+00:00 locale=en></span></span><span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-iso_date" title="April 29, 2025 00:46:39 UTC">2025-04-29</span> </span> <span class=md-source-file__fact> <span class=md-icon title=Created> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M14.47 15.08 11 13V7h1.5v5.25l3.08 1.83c-.41.28-.79.62-1.11 1m-1.39 4.84c-.36.05-.71.08-1.08.08-4.42 0-8-3.58-8-8s3.58-8 8-8 8 3.58 8 8c0 .37-.03.72-.08 1.08.69.1 1.33.32 1.92.64.1-.56.16-1.13.16-1.72 0-5.5-4.5-10-10-10S2 6.5 2 12s4.47 10 10 10c.59 0 1.16-.06 1.72-.16-.32-.59-.54-1.23-.64-1.92M18 15v3h-3v2h3v3h2v-3h3v-2h-3v-3z"></path></svg> </span> <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-timeago" title="March 10, 2025 15:19:30 UTC"><span class=timeago datetime=2025-03-10T15:19:30+00:00 locale=en></span></span><span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-iso_date" title="March 10, 2025 15:19:30 UTC">2025-03-10</span> </span> </aside> </article> </div> <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"></path></svg> Back to top </button> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> <div class=ucar_links> <span>¬© 2024 UCAR</span> <span><a href=https://www.ucar.edu/accessibility>Accessibility</a></span> <span><a href=https://www.ucar.edu/cookie-other-tracking-technologies-notice>Cookies</a></span> <span><a href=https://www.ucar.edu/notification-copyright-infringement-digital-millenium-copyright-act>Copyright Issues</a></span> <span><a href=https://www.ucar.edu/who-we-are/ethics-integrity/nondiscrimination>Nondiscrimination</a></span> <span><a href=https://www.ucar.edu/privacy-notice>Privacy</a></span> <span><a href=https://www.ucar.edu/terms-of-use>Terms of Use</a></span> <span><a href=https://ncar.ucar.edu/ >NSF NCAR Home</a></span> <span><a href=https://www.ucar.edu/ >UCAR Home</a></span> </div> <p class=bold>Postal Address:</p> P.O. Box 3000, Boulder, CO 80307-3000 ‚Ä¢ <p class=bold>Shipping Address:</p> 3090 Center Green Drive, Boulder, CO 80301 </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://sea.ucar.edu target=_blank rel=noopener title=sea.ucar.edu class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19.07 4.93C17.22 3 14.66 1.96 12 2c-2.66-.04-5.21 1-7.06 2.93C3 6.78 1.96 9.34 2 12c-.04 2.66 1 5.21 2.93 7.06C6.78 21 9.34 22.04 12 22c2.66.04 5.21-1 7.06-2.93C21 17.22 22.04 14.66 22 12c.04-2.66-1-5.22-2.93-7.07M17 12v6h-3.5v-5h-3v5H7v-6H5l7-7 7.5 7z"></path></svg> </a> <a href=https://ucarsea.slack.com target=_blank rel=noopener title=ucarsea.slack.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M6 15a2 2 0 0 1-2 2 2 2 0 0 1-2-2 2 2 0 0 1 2-2h2zm1 0a2 2 0 0 1 2-2 2 2 0 0 1 2 2v5a2 2 0 0 1-2 2 2 2 0 0 1-2-2zm2-8a2 2 0 0 1-2-2 2 2 0 0 1 2-2 2 2 0 0 1 2 2v2zm0 1a2 2 0 0 1 2 2 2 2 0 0 1-2 2H4a2 2 0 0 1-2-2 2 2 0 0 1 2-2zm8 2a2 2 0 0 1 2-2 2 2 0 0 1 2 2 2 2 0 0 1-2 2h-2zm-1 0a2 2 0 0 1-2 2 2 2 0 0 1-2-2V5a2 2 0 0 1 2-2 2 2 0 0 1 2 2zm-2 8a2 2 0 0 1 2 2 2 2 0 0 1-2 2 2 2 0 0 1-2-2v-2zm0-1a2 2 0 0 1-2-2 2 2 0 0 1 2-2h5a2 2 0 0 1 2 2 2 2 0 0 1-2 2z"></path></svg> </a> <a href=mailto:sea_committee@ucar.edu target=_blank rel=noopener title class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="m20 8-8 5-8-5V6l8 5 8-5m0-2H4c-1.11 0-2 .89-2 2v12a2 2 0 0 0 2 2h16a2 2 0 0 0 2-2V6a2 2 0 0 0-2-2"></path></svg> </a> </div> </div> </div> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <table class=md-footer-meta__inner> <colgroup> <col style="width: 20%"> <col style="width: 80%"> </colgroup> <tbody> <tr> <td valign=center> <img alt="National Science Foundation" src=https://ncar.ucar.edu/profiles/custom/ncar_ucar_umbrella/themes/custom/koru/libraries/koru-base/img/NSF_Official_logo.png width=290> </td> <td valign=center> <p style="font-size:0.85em; "> This material is based upon work supported by the NSF National Center for Atmospheric Research, a major facility sponsored by the U.S. National Science Foundation and managed by the University Corporation for Atmospheric Research. Any opinions, findings and conclusions or recommendations expressed in this material do not necessarily reflect the views of the <a href=https://nsf.gov/ >U.S. National Science Foundation</a>. </p> </td> </tr> </tbody> </table> </div> </div> </footer></div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "../..", "features": ["search.suggest", "search.highlight", "content.tabs.link", "content.code.annotation", "content.code.annotate", "content.code.copy", "navigation.expand", "navigation.footer", "navigation.indexes", "navigation.top", "navigation.path", "navigation.tabs", "navigation.tabs.sticky", "navigation.tracking", "navigation.sections", "search.highlight", "search.share", "search.suggest", "toc.integrate", "announce.dismiss"], "search": "../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script> <script src=../../assets/javascripts/bundle.f55a23d4.min.js></script> <script src=../../js/timeago.min.js></script> <script src=../../js/timeago_mkdocs_material.js></script> <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script> <script src=../../js/open_in_new_tab.js></script> <script id=init-glightbox>const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(()=>{ lightbox.reload(); });
</script></body></html>